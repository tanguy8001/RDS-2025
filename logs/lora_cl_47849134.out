no change     /cluster/home/tdieudonne/miniconda3/condabin/conda
no change     /cluster/home/tdieudonne/miniconda3/bin/conda
no change     /cluster/home/tdieudonne/miniconda3/bin/conda-env
no change     /cluster/home/tdieudonne/miniconda3/bin/activate
no change     /cluster/home/tdieudonne/miniconda3/bin/deactivate
no change     /cluster/home/tdieudonne/miniconda3/etc/profile.d/conda.sh
no change     /cluster/home/tdieudonne/miniconda3/etc/fish/conf.d/conda.fish
no change     /cluster/home/tdieudonne/miniconda3/shell/condabin/Conda.psm1
no change     /cluster/home/tdieudonne/miniconda3/shell/condabin/conda-hook.ps1
no change     /cluster/home/tdieudonne/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh
no change     /cluster/home/tdieudonne/miniconda3/etc/profile.d/conda.csh
no change     /cluster/home/tdieudonne/.bashrc
No action taken.
Many modules are hidden in this stack. Use "module --show_hidden spider SOFTWARE" if you are not able to find the required software
seed [1993]
devices_type ['0']
2025-11-05 17:51:29,245 [trainer.py] => config: ./exps/sdlora_c100.json
2025-11-05 17:51:29,245 [trainer.py] => seed: 1993
2025-11-05 17:51:29,245 [trainer.py] => prefix: reproduce
2025-11-05 17:51:29,245 [trainer.py] => dataset: cifar224
2025-11-05 17:51:29,245 [trainer.py] => memory_size: 0
2025-11-05 17:51:29,245 [trainer.py] => memory_per_class: 0
2025-11-05 17:51:29,246 [trainer.py] => fixed_memory: False
2025-11-05 17:51:29,246 [trainer.py] => shuffle: True
2025-11-05 17:51:29,246 [trainer.py] => init_cls: 10
2025-11-05 17:51:29,246 [trainer.py] => increment: 10
2025-11-05 17:51:29,246 [trainer.py] => model_name: sdlora
2025-11-05 17:51:29,246 [trainer.py] => backbone_type: vit_base_patch16_224
2025-11-05 17:51:29,246 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-05 17:51:29,246 [trainer.py] => scheduler: cosine
2025-11-05 17:51:29,246 [trainer.py] => min_lr: 0.005
2025-11-05 17:51:29,246 [trainer.py] => tuned_epoch: 20
2025-11-05 17:51:29,246 [trainer.py] => filepath: ./CF100/
2025-11-05 17:51:29,246 [trainer.py] => init_epoch: 20
2025-11-05 17:51:29,246 [trainer.py] => init_lr: 0.008
2025-11-05 17:51:29,246 [trainer.py] => init_milestones: [60, 120, 170]
2025-11-05 17:51:29,246 [trainer.py] => init_lr_decay: 0.1
2025-11-05 17:51:29,246 [trainer.py] => init_weight_decay: 0.0005
2025-11-05 17:51:29,246 [trainer.py] => epochs: 20
2025-11-05 17:51:29,246 [trainer.py] => lrate: 0.008
2025-11-05 17:51:29,246 [trainer.py] => milestones: [40, 70]
2025-11-05 17:51:29,246 [trainer.py] => lrate_decay: 0
2025-11-05 17:51:29,246 [trainer.py] => batch_size: 128
2025-11-05 17:51:29,246 [trainer.py] => weight_decay: 0.0002
2025-11-05 17:51:29,246 [trainer.py] => _comment_gumbel: Gumbel CL-LoRA hyperparameters
2025-11-05 17:51:29,246 [trainer.py] => gumbel_tau_init: 5.0
2025-11-05 17:51:29,246 [trainer.py] => gumbel_tau_final: 0.5
2025-11-05 17:51:29,246 [trainer.py] => gumbel_anneal_rate: 0.999
2025-11-05 17:51:29,246 [trainer.py] => lambda_sparsity: 0.01
2025-11-05 17:51:29,246 [trainer.py] => growth_threshold: 0.1
Files already downloaded and verified
Files already downloaded and verified
2025-11-05 17:51:31,544 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
!!!!!!! multiple_gpus [device(type='cuda', index=0)]
This is for the BaseNet initialization.
2025-11-05 17:51:37,896 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 17:51:38,116 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
Initialize task-id and curtask id
After BaseNet initialization.
2025-11-05 17:51:40,355 [trainer.py] => [RESUME] Task 0 already completed, will resume from task 1
2025-11-05 17:51:40,358 [trainer.py] => [RESUME] Task 1 already completed, will resume from task 2
2025-11-05 17:51:40,361 [trainer.py] => [RESUME] Task 2 already completed, will resume from task 3
2025-11-05 17:51:40,364 [trainer.py] => [RESUME] Task 3 already completed, will resume from task 4
2025-11-05 17:51:40,366 [trainer.py] => [RESUME] Task 4 already completed, will resume from task 5
2025-11-05 17:51:40,368 [trainer.py] => [RESUME] Task 5 already completed, will resume from task 6
2025-11-05 17:51:40,370 [trainer.py] => [RESUME] Task 6 already completed, will resume from task 7
2025-11-05 17:51:40,373 [trainer.py] => [RESUME] Task 7 already completed, will resume from task 8
2025-11-05 17:51:40,376 [trainer.py] => [RESUME] Task 8 already completed, will resume from task 9
2025-11-05 17:51:40,380 [trainer.py] => [RESUME] Task 9 already completed, will resume from task 10
2025-11-05 17:51:40,381 [trainer.py] => [RESUME] Resuming training from task 10
