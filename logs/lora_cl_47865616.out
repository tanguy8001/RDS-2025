no change     /cluster/home/tdieudonne/miniconda3/condabin/conda
no change     /cluster/home/tdieudonne/miniconda3/bin/conda
no change     /cluster/home/tdieudonne/miniconda3/bin/conda-env
no change     /cluster/home/tdieudonne/miniconda3/bin/activate
no change     /cluster/home/tdieudonne/miniconda3/bin/deactivate
no change     /cluster/home/tdieudonne/miniconda3/etc/profile.d/conda.sh
no change     /cluster/home/tdieudonne/miniconda3/etc/fish/conf.d/conda.fish
no change     /cluster/home/tdieudonne/miniconda3/shell/condabin/Conda.psm1
no change     /cluster/home/tdieudonne/miniconda3/shell/condabin/conda-hook.ps1
no change     /cluster/home/tdieudonne/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh
no change     /cluster/home/tdieudonne/miniconda3/etc/profile.d/conda.csh
no change     /cluster/home/tdieudonne/.bashrc
No action taken.
Many modules are hidden in this stack. Use "module --show_hidden spider SOFTWARE" if you are not able to find the required software
seed [1993]
devices_type ['0']
2025-11-05 21:03:14,574 [trainer.py] => config: ./exps/sdlora_c100.json
2025-11-05 21:03:14,574 [trainer.py] => seed: 1993
2025-11-05 21:03:14,574 [trainer.py] => prefix: reproduce
2025-11-05 21:03:14,575 [trainer.py] => dataset: cifar224
2025-11-05 21:03:14,575 [trainer.py] => memory_size: 0
2025-11-05 21:03:14,575 [trainer.py] => memory_per_class: 0
2025-11-05 21:03:14,575 [trainer.py] => fixed_memory: False
2025-11-05 21:03:14,575 [trainer.py] => shuffle: True
2025-11-05 21:03:14,575 [trainer.py] => init_cls: 10
2025-11-05 21:03:14,575 [trainer.py] => increment: 10
2025-11-05 21:03:14,575 [trainer.py] => model_name: sdlora
2025-11-05 21:03:14,575 [trainer.py] => backbone_type: vit_base_patch16_224
2025-11-05 21:03:14,575 [trainer.py] => device: [device(type='cuda', index=0)]
2025-11-05 21:03:14,575 [trainer.py] => scheduler: cosine
2025-11-05 21:03:14,575 [trainer.py] => min_lr: 0.005
2025-11-05 21:03:14,575 [trainer.py] => tuned_epoch: 20
2025-11-05 21:03:14,575 [trainer.py] => filepath: ./CF100/
2025-11-05 21:03:14,575 [trainer.py] => init_epoch: 20
2025-11-05 21:03:14,575 [trainer.py] => init_lr: 0.008
2025-11-05 21:03:14,575 [trainer.py] => init_milestones: [60, 120, 170]
2025-11-05 21:03:14,575 [trainer.py] => init_lr_decay: 0.1
2025-11-05 21:03:14,575 [trainer.py] => init_weight_decay: 0.0005
2025-11-05 21:03:14,575 [trainer.py] => epochs: 20
2025-11-05 21:03:14,575 [trainer.py] => lrate: 0.008
2025-11-05 21:03:14,576 [trainer.py] => milestones: [40, 70]
2025-11-05 21:03:14,576 [trainer.py] => lrate_decay: 0
2025-11-05 21:03:14,576 [trainer.py] => batch_size: 128
2025-11-05 21:03:14,576 [trainer.py] => weight_decay: 0.0002
2025-11-05 21:03:14,576 [trainer.py] => _comment_gumbel: Gumbel CL-LoRA hyperparameters
2025-11-05 21:03:14,576 [trainer.py] => gumbel_tau_init: 5.0
2025-11-05 21:03:14,576 [trainer.py] => gumbel_tau_final: 0.5
2025-11-05 21:03:14,576 [trainer.py] => gumbel_anneal_rate: 0.999
2025-11-05 21:03:14,576 [trainer.py] => lambda_sparsity: 0.001
2025-11-05 21:03:14,576 [trainer.py] => growth_threshold: 0.1
Files already downloaded and verified
Files already downloaded and verified
2025-11-05 21:03:17,184 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
!!!!!!! multiple_gpus [device(type='cuda', index=0)]
This is for the BaseNet initialization.
2025-11-05 21:03:24,528 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 21:03:24,792 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
Initialize task-id and curtask id
After BaseNet initialization.
task 0
2025-11-05 21:03:27,059 [trainer.py] => All params: 171965992
2025-11-05 21:03:27,061 [trainer.py] => Trainable params: 368680
2025-11-05 21:03:27,061 [sdlora.py] => Learning on 0-10
/cluster/home/tdieudonne/miniconda3/envs/rds/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
  0%|          | 0/20 [00:00<?, ?it/s]Task 0, Epoch 1/20 => Loss 0.815, Train_accy 74.18, Test_accy 95.00:   0%|          | 0/20 [00:34<?, ?it/s]Task 0, Epoch 1/20 => Loss 0.815, Train_accy 74.18, Test_accy 95.00:   5%|â–Œ         | 1/20 [00:34<10:56, 34.54s/it]Task 0, Epoch 2/20 => Loss 0.400, Train_accy 88.20:   5%|â–Œ         | 1/20 [01:01<10:56, 34.54s/it]                 Task 0, Epoch 2/20 => Loss 0.400, Train_accy 88.20:  10%|â–ˆ         | 2/20 [01:01<09:06, 30.35s/it]Task 0, Epoch 3/20 => Loss 0.363, Train_accy 89.06:  10%|â–ˆ         | 2/20 [01:29<09:06, 30.35s/it]Task 0, Epoch 3/20 => Loss 0.363, Train_accy 89.06:  15%|â–ˆâ–Œ        | 3/20 [01:29<08:13, 29.06s/it]Task 0, Epoch 4/20 => Loss 0.284, Train_accy 91.14:  15%|â–ˆâ–Œ        | 3/20 [01:57<08:13, 29.06s/it]Task 0, Epoch 4/20 => Loss 0.284, Train_accy 91.14:  20%|â–ˆâ–ˆ        | 4/20 [01:57<07:38, 28.63s/it]Task 0, Epoch 5/20 => Loss 0.248, Train_accy 92.22:  20%|â–ˆâ–ˆ        | 4/20 [02:25<07:38, 28.63s/it]Task 0, Epoch 5/20 => Loss 0.248, Train_accy 92.22:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:25<07:03, 28.24s/it]Task 0, Epoch 6/20 => Loss 0.251, Train_accy 92.30, Test_accy 98.40:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:56<07:03, 28.24s/it]Task 0, Epoch 6/20 => Loss 0.251, Train_accy 92.30, Test_accy 98.40:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:56<06:51, 29.36s/it]Task 0, Epoch 7/20 => Loss 0.247, Train_accy 92.14:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [03:24<06:51, 29.36s/it]                 Task 0, Epoch 7/20 => Loss 0.247, Train_accy 92.14:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [03:24<06:14, 28.84s/it]Task 0, Epoch 8/20 => Loss 0.251, Train_accy 92.24:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [03:52<06:14, 28.84s/it]Task 0, Epoch 8/20 => Loss 0.251, Train_accy 92.24:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [03:52<05:42, 28.54s/it]Task 0, Epoch 9/20 => Loss 0.241, Train_accy 92.52:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [04:19<05:42, 28.54s/it]Task 0, Epoch 9/20 => Loss 0.241, Train_accy 92.52:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [04:19<05:10, 28.25s/it]Task 0, Epoch 10/20 => Loss 0.216, Train_accy 93.14:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [04:47<05:10, 28.25s/it]Task 0, Epoch 10/20 => Loss 0.216, Train_accy 93.14:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [04:47<04:40, 28.01s/it]Task 0, Epoch 11/20 => Loss 0.209, Train_accy 92.96, Test_accy 98.70:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [05:19<04:40, 28.01s/it]Task 0, Epoch 11/20 => Loss 0.209, Train_accy 92.96, Test_accy 98.70:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [05:19<04:22, 29.20s/it]Task 0, Epoch 12/20 => Loss 0.214, Train_accy 93.08:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [05:47<04:22, 29.20s/it]                 Task 0, Epoch 12/20 => Loss 0.214, Train_accy 93.08:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [05:47<03:50, 28.78s/it]Task 0, Epoch 13/20 => Loss 0.221, Train_accy 93.04:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [06:14<03:50, 28.78s/it]Task 0, Epoch 13/20 => Loss 0.221, Train_accy 93.04:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [06:14<03:18, 28.41s/it]Task 0, Epoch 14/20 => Loss 0.200, Train_accy 93.38:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [06:42<03:18, 28.41s/it]Task 0, Epoch 14/20 => Loss 0.200, Train_accy 93.38:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [06:42<02:49, 28.25s/it]Task 0, Epoch 15/20 => Loss 0.197, Train_accy 93.54:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [07:10<02:49, 28.25s/it]Task 0, Epoch 15/20 => Loss 0.197, Train_accy 93.54:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [07:10<02:20, 28.15s/it]Task 0, Epoch 16/20 => Loss 0.181, Train_accy 93.94, Test_accy 98.90:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [07:42<02:20, 28.15s/it]Task 0, Epoch 16/20 => Loss 0.181, Train_accy 93.94, Test_accy 98.90:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [07:42<01:56, 29.22s/it]Task 0, Epoch 17/20 => Loss 0.213, Train_accy 93.40:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [08:09<01:56, 29.22s/it]                 Task 0, Epoch 17/20 => Loss 0.213, Train_accy 93.40:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [08:09<01:26, 28.80s/it]Task 0, Epoch 18/20 => Loss 0.193, Train_accy 93.74:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [08:37<01:26, 28.80s/it]Task 0, Epoch 18/20 => Loss 0.193, Train_accy 93.74:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [08:37<00:56, 28.38s/it]Task 0, Epoch 19/20 => Loss 0.199, Train_accy 93.56:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [09:05<00:56, 28.38s/it]Task 0, Epoch 19/20 => Loss 0.199, Train_accy 93.56:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [09:05<00:28, 28.19s/it]Task 0, Epoch 20/20 => Loss 0.181, Train_accy 94.24:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [09:32<00:28, 28.19s/it]Task 0, Epoch 20/20 => Loss 0.181, Train_accy 94.24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:32<00:00, 28.11s/it]Task 0, Epoch 20/20 => Loss 0.181, Train_accy 94.24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [09:32<00:00, 28.65s/it]
2025-11-05 21:13:00,110 [sdlora.py] => Task 0, Epoch 20/20 => Loss 0.181, Train_accy 94.24
2025-11-05 21:13:04,314 [trainer.py] => No NME accuracy.
2025-11-05 21:13:04,314 [trainer.py] => CNN: {'total': np.float64(99.0), '00-09': np.float64(99.0), 'old': 0, 'new': np.float64(99.0)}
2025-11-05 21:13:04,314 [trainer.py] => CNN top1 curve: [np.float64(99.0)]
2025-11-05 21:13:04,314 [trainer.py] => CNN top5 curve: [np.float64(100.0)]

Average Accuracy (CNN): 99.0
2025-11-05 21:13:04,315 [trainer.py] => Average Accuracy (CNN): 99.0 

task 1
2025-11-05 21:13:04,316 [trainer.py] => All params: 171973682
2025-11-05 21:13:04,317 [trainer.py] => Trainable params: 376370
2025-11-05 21:13:04,318 [sdlora.py] => Learning on 10-20
2025-11-05 21:13:05,213 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 21:13:05,415 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
/cluster/home/tdieudonne/SD-Lora-CL/backbone/lora.py:515: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_lora_A['saved_A_'+str(i)] = torch.load(file_path)
/cluster/home/tdieudonne/SD-Lora-CL/backbone/lora.py:517: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_lora_B['saved_B_'+str(i)] = torch.load(file_path)
save_file ./CF100/
2025-11-05 21:13:05,609 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]/cluster/home/tdieudonne/SD-Lora-CL/backbone/lora.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  w_As = torch.load(file_path)
Task 1, Epoch 1/20 => Loss 0.740 (clf=0.740, sparse=0.0000), tau=4.828, Train_accy 61.56, Test_accy 94.75:   0%|          | 0/20 [00:37<?, ?it/s]Task 1, Epoch 1/20 => Loss 0.740 (clf=0.740, sparse=0.0000), tau=4.828, Train_accy 61.56, Test_accy 94.75:   5%|â–Œ         | 1/20 [00:37<11:47, 37.25s/it]Task 1, Epoch 2/20 => Loss 0.356 (clf=0.356, sparse=0.0000), tau=4.658, Train_accy 82.30:   5%|â–Œ         | 1/20 [01:06<11:47, 37.25s/it]                 Task 1, Epoch 2/20 => Loss 0.356 (clf=0.356, sparse=0.0000), tau=4.658, Train_accy 82.30:  10%|â–ˆ         | 2/20 [01:06<09:49, 32.76s/it]Task 1, Epoch 3/20 => Loss 0.318 (clf=0.318, sparse=0.0000), tau=4.495, Train_accy 80.92:  10%|â–ˆ         | 2/20 [01:36<09:49, 32.76s/it]Task 1, Epoch 3/20 => Loss 0.318 (clf=0.318, sparse=0.0000), tau=4.495, Train_accy 80.92:  15%|â–ˆâ–Œ        | 3/20 [01:36<08:51, 31.27s/it]Task 1, Epoch 4/20 => Loss 0.303 (clf=0.303, sparse=0.0000), tau=4.338, Train_accy 80.88:  15%|â–ˆâ–Œ        | 3/20 [02:05<08:51, 31.27s/it]Task 1, Epoch 4/20 => Loss 0.303 (clf=0.303, sparse=0.0000), tau=4.338, Train_accy 80.88:  20%|â–ˆâ–ˆ        | 4/20 [02:05<08:08, 30.55s/it]Task 1, Epoch 5/20 => Loss 0.277 (clf=0.277, sparse=0.0000), tau=4.188, Train_accy 80.68:  20%|â–ˆâ–ˆ        | 4/20 [02:35<08:08, 30.55s/it]Task 1, Epoch 5/20 => Loss 0.277 (clf=0.277, sparse=0.0000), tau=4.188, Train_accy 80.68:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:35<07:32, 30.17s/it]Task 1, Epoch 6/20 => Loss 0.271 (clf=0.271, sparse=0.0000), tau=4.043, Train_accy 82.18, Test_accy 95.85:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:11<07:32, 30.17s/it]Task 1, Epoch 6/20 => Loss 0.271 (clf=0.271, sparse=0.0000), tau=4.043, Train_accy 82.18, Test_accy 95.85:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [03:11<07:31, 32.28s/it]Task 1, Epoch 7/20 => Loss 0.274 (clf=0.274, sparse=0.0000), tau=3.904, Train_accy 82.28:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [03:41<07:31, 32.28s/it]                 Task 1, Epoch 7/20 => Loss 0.274 (clf=0.274, sparse=0.0000), tau=3.904, Train_accy 82.28:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [03:41<06:48, 31.40s/it]Task 1, Epoch 8/20 => Loss 0.266 (clf=0.266, sparse=0.0000), tau=3.770, Train_accy 81.78:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [04:10<06:48, 31.40s/it]Task 1, Epoch 8/20 => Loss 0.266 (clf=0.266, sparse=0.0000), tau=3.770, Train_accy 81.78:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [04:10<06:09, 30.80s/it]Task 1, Epoch 9/20 => Loss 0.252 (clf=0.252, sparse=0.0000), tau=3.642, Train_accy 82.64:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [04:40<06:09, 30.80s/it]Task 1, Epoch 9/20 => Loss 0.252 (clf=0.252, sparse=0.0000), tau=3.642, Train_accy 82.64:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [04:40<05:34, 30.40s/it]Task 1, Epoch 10/20 => Loss 0.268 (clf=0.268, sparse=0.0000), tau=3.519, Train_accy 82.76:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [05:09<05:34, 30.40s/it]Task 1, Epoch 10/20 => Loss 0.268 (clf=0.268, sparse=0.0000), tau=3.519, Train_accy 82.76:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [05:09<05:01, 30.14s/it]Task 1, Epoch 11/20 => Loss 0.253 (clf=0.253, sparse=0.0000), tau=3.400, Train_accy 82.26, Test_accy 96.00:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [05:46<05:01, 30.14s/it]Task 1, Epoch 11/20 => Loss 0.253 (clf=0.253, sparse=0.0000), tau=3.400, Train_accy 82.26, Test_accy 96.00:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [05:46<04:48, 32.09s/it]Task 1, Epoch 12/20 => Loss 0.245 (clf=0.245, sparse=0.0000), tau=3.287, Train_accy 82.86:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [06:15<04:48, 32.09s/it]                 Task 1, Epoch 12/20 => Loss 0.245 (clf=0.245, sparse=0.0000), tau=3.287, Train_accy 82.86:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [06:15<04:10, 31.31s/it]Task 1, Epoch 13/20 => Loss 0.236 (clf=0.236, sparse=0.0000), tau=3.177, Train_accy 83.22:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [06:45<04:10, 31.31s/it]Task 1, Epoch 13/20 => Loss 0.236 (clf=0.236, sparse=0.0000), tau=3.177, Train_accy 83.22:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [06:45<03:35, 30.82s/it]Task 1, Epoch 14/20 => Loss 0.234 (clf=0.234, sparse=0.0000), tau=3.072, Train_accy 84.18:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [07:14<03:35, 30.82s/it]Task 1, Epoch 14/20 => Loss 0.234 (clf=0.234, sparse=0.0000), tau=3.072, Train_accy 84.18:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [07:14<03:02, 30.37s/it]Task 1, Epoch 15/20 => Loss 0.247 (clf=0.247, sparse=0.0000), tau=2.971, Train_accy 84.40:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [07:44<03:02, 30.37s/it]Task 1, Epoch 15/20 => Loss 0.247 (clf=0.247, sparse=0.0000), tau=2.971, Train_accy 84.40:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [07:44<02:30, 30.12s/it]Task 1, Epoch 16/20 => Loss 0.243 (clf=0.243, sparse=0.0000), tau=2.874, Train_accy 83.86, Test_accy 96.05:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [08:20<02:30, 30.12s/it]Task 1, Epoch 16/20 => Loss 0.243 (clf=0.243, sparse=0.0000), tau=2.874, Train_accy 83.86, Test_accy 96.05:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [08:20<02:07, 31.97s/it]Task 1, Epoch 17/20 => Loss 0.259 (clf=0.259, sparse=0.0000), tau=2.781, Train_accy 84.00:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [08:50<02:07, 31.97s/it]                 Task 1, Epoch 17/20 => Loss 0.259 (clf=0.259, sparse=0.0000), tau=2.781, Train_accy 84.00:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [08:50<01:33, 31.16s/it]Task 1, Epoch 18/20 => Loss 0.234 (clf=0.234, sparse=0.0000), tau=2.692, Train_accy 84.28:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [09:19<01:33, 31.16s/it]Task 1, Epoch 18/20 => Loss 0.234 (clf=0.234, sparse=0.0000), tau=2.692, Train_accy 84.28:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [09:19<01:01, 30.55s/it]Task 1, Epoch 19/20 => Loss 0.222 (clf=0.222, sparse=0.0000), tau=2.606, Train_accy 84.96:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [09:48<01:01, 30.55s/it]Task 1, Epoch 19/20 => Loss 0.222 (clf=0.222, sparse=0.0000), tau=2.606, Train_accy 84.96:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [09:48<00:30, 30.22s/it]Task 1, Epoch 20/20 => Loss 0.225 (clf=0.225, sparse=0.0000), tau=2.523, Train_accy 85.42:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [10:18<00:30, 30.22s/it]Task 1, Epoch 20/20 => Loss 0.225 (clf=0.225, sparse=0.0000), tau=2.523, Train_accy 85.42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:18<00:00, 30.02s/it]Task 1, Epoch 20/20 => Loss 0.225 (clf=0.225, sparse=0.0000), tau=2.523, Train_accy 85.42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:18<00:00, 30.91s/it]
2025-11-05 21:23:23,748 [sdlora.py] => Task 1, Epoch 20/20 => Loss 0.225 (clf=0.225, sparse=0.0000), tau=2.523, Train_accy 85.42
2025-11-05 21:23:23,751 [sdlora.py] => 
[Conditional Growth] Task 1 - Beta values: [0.5 0.5]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Keeping task 1 (beta above threshold)
2025-11-05 21:23:23,751 [sdlora.py] => [Conditional Growth] âœ“ Keeping NEW adapter for task 1 (beta=0.5000 > 0.1)
2025-11-05 21:23:23,756 [sdlora.py] => [Conditional Growth] Summary: 2 kept, 0 pruned this round
2025-11-05 21:23:23,756 [sdlora.py] => [Conditional Growth] Active adapters: 2/2 (100.0%)
2025-11-05 21:23:23,765 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_1.pt
2025-11-05 21:23:30,737 [trainer.py] => No NME accuracy.
2025-11-05 21:23:30,738 [trainer.py] => CNN: {'total': np.float64(96.35), '00-09': np.float64(97.1), '10-19': np.float64(95.6), 'old': np.float64(97.1), 'new': np.float64(95.6)}
2025-11-05 21:23:30,738 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35)]
2025-11-05 21:23:30,738 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85)]

Average Accuracy (CNN): 97.675
2025-11-05 21:23:30,738 [trainer.py] => Average Accuracy (CNN): 97.675 

task 2
2025-11-05 21:23:30,740 [trainer.py] => All params: 171981372
2025-11-05 21:23:30,741 [trainer.py] => Trainable params: 384060
2025-11-05 21:23:30,741 [sdlora.py] => Learning on 20-30
2025-11-05 21:23:31,623 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 21:23:31,786 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 21:23:32,051 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 2, Epoch 1/20 => Loss 0.599 (clf=0.593, sparse=5.8340), tau=4.828, Train_accy 61.26, Test_accy 91.97:   0%|          | 0/20 [00:40<?, ?it/s]Task 2, Epoch 1/20 => Loss 0.599 (clf=0.593, sparse=5.8340), tau=4.828, Train_accy 61.26, Test_accy 91.97:   5%|â–Œ         | 1/20 [00:40<12:46, 40.34s/it]Task 2, Epoch 2/20 => Loss 0.236 (clf=0.230, sparse=5.8645), tau=4.658, Train_accy 86.18:   5%|â–Œ         | 1/20 [01:11<12:46, 40.34s/it]                 Task 2, Epoch 2/20 => Loss 0.236 (clf=0.230, sparse=5.8645), tau=4.658, Train_accy 86.18:  10%|â–ˆ         | 2/20 [01:11<10:26, 34.80s/it]Task 2, Epoch 3/20 => Loss 0.211 (clf=0.205, sparse=5.9172), tau=4.495, Train_accy 85.68:  10%|â–ˆ         | 2/20 [01:41<10:26, 34.80s/it]Task 2, Epoch 3/20 => Loss 0.211 (clf=0.205, sparse=5.9172), tau=4.495, Train_accy 85.68:  15%|â–ˆâ–Œ        | 3/20 [01:41<09:19, 32.90s/it]Task 2, Epoch 4/20 => Loss 0.188 (clf=0.182, sparse=5.5518), tau=4.338, Train_accy 85.94:  15%|â–ˆâ–Œ        | 3/20 [02:12<09:19, 32.90s/it]Task 2, Epoch 4/20 => Loss 0.188 (clf=0.182, sparse=5.5518), tau=4.338, Train_accy 85.94:  20%|â–ˆâ–ˆ        | 4/20 [02:12<08:33, 32.09s/it]Task 2, Epoch 5/20 => Loss 0.182 (clf=0.176, sparse=5.5653), tau=4.188, Train_accy 86.14:  20%|â–ˆâ–ˆ        | 4/20 [02:43<08:33, 32.09s/it]Task 2, Epoch 5/20 => Loss 0.182 (clf=0.176, sparse=5.5653), tau=4.188, Train_accy 86.14:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:43<07:54, 31.65s/it]Task 2, Epoch 6/20 => Loss 0.182 (clf=0.177, sparse=5.6579), tau=4.043, Train_accy 85.74, Test_accy 93.90:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:24<07:54, 31.65s/it]Task 2, Epoch 6/20 => Loss 0.182 (clf=0.177, sparse=5.6579), tau=4.043, Train_accy 85.74, Test_accy 93.90:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [03:24<08:06, 34.74s/it]Task 2, Epoch 7/20 => Loss 0.179 (clf=0.173, sparse=5.6800), tau=3.904, Train_accy 85.74:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [03:55<08:06, 34.74s/it]                 Task 2, Epoch 7/20 => Loss 0.179 (clf=0.173, sparse=5.6800), tau=3.904, Train_accy 85.74:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [03:55<07:15, 33.47s/it]Task 2, Epoch 8/20 => Loss 0.180 (clf=0.174, sparse=5.4973), tau=3.770, Train_accy 86.34:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [04:26<07:15, 33.47s/it]Task 2, Epoch 8/20 => Loss 0.180 (clf=0.174, sparse=5.4973), tau=3.770, Train_accy 86.34:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [04:26<06:31, 32.67s/it]Task 2, Epoch 9/20 => Loss 0.153 (clf=0.147, sparse=5.4067), tau=3.642, Train_accy 86.58:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [04:56<06:31, 32.67s/it]Task 2, Epoch 9/20 => Loss 0.153 (clf=0.147, sparse=5.4067), tau=3.642, Train_accy 86.58:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [04:56<05:53, 32.09s/it]Task 2, Epoch 10/20 => Loss 0.170 (clf=0.164, sparse=5.7473), tau=3.519, Train_accy 86.42:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [05:27<05:53, 32.09s/it]Task 2, Epoch 10/20 => Loss 0.170 (clf=0.164, sparse=5.7473), tau=3.519, Train_accy 86.42:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [05:27<05:17, 31.73s/it]Task 2, Epoch 11/20 => Loss 0.161 (clf=0.156, sparse=5.4048), tau=3.400, Train_accy 85.96, Test_accy 94.30:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [06:08<05:17, 31.73s/it]Task 2, Epoch 11/20 => Loss 0.161 (clf=0.156, sparse=5.4048), tau=3.400, Train_accy 85.96, Test_accy 94.30:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [06:08<05:10, 34.48s/it]Task 2, Epoch 12/20 => Loss 0.147 (clf=0.142, sparse=5.5746), tau=3.287, Train_accy 87.22:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [06:39<05:10, 34.48s/it]                 Task 2, Epoch 12/20 => Loss 0.147 (clf=0.142, sparse=5.5746), tau=3.287, Train_accy 87.22:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [06:39<04:26, 33.37s/it]Task 2, Epoch 13/20 => Loss 0.151 (clf=0.146, sparse=5.5877), tau=3.177, Train_accy 87.20:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [07:10<04:26, 33.37s/it]Task 2, Epoch 13/20 => Loss 0.151 (clf=0.146, sparse=5.5877), tau=3.177, Train_accy 87.20:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [07:10<03:48, 32.62s/it]Task 2, Epoch 14/20 => Loss 0.157 (clf=0.152, sparse=5.3776), tau=3.072, Train_accy 88.56:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [07:41<03:48, 32.62s/it]Task 2, Epoch 14/20 => Loss 0.157 (clf=0.152, sparse=5.3776), tau=3.072, Train_accy 88.56:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [07:41<03:12, 32.13s/it]Task 2, Epoch 15/20 => Loss 0.160 (clf=0.155, sparse=4.8459), tau=2.971, Train_accy 86.92:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [08:12<03:12, 32.13s/it]Task 2, Epoch 15/20 => Loss 0.160 (clf=0.155, sparse=4.8459), tau=2.971, Train_accy 86.92:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [08:12<02:38, 31.71s/it]Task 2, Epoch 16/20 => Loss 0.145 (clf=0.140, sparse=5.3904), tau=2.874, Train_accy 88.44, Test_accy 94.13:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [08:52<02:38, 31.71s/it]Task 2, Epoch 16/20 => Loss 0.145 (clf=0.140, sparse=5.3904), tau=2.874, Train_accy 88.44, Test_accy 94.13:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [08:52<02:17, 34.41s/it]Task 2, Epoch 17/20 => Loss 0.148 (clf=0.144, sparse=4.8345), tau=2.781, Train_accy 87.94:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [09:23<02:17, 34.41s/it]                 Task 2, Epoch 17/20 => Loss 0.148 (clf=0.144, sparse=4.8345), tau=2.781, Train_accy 87.94:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [09:23<01:39, 33.33s/it]Task 2, Epoch 18/20 => Loss 0.149 (clf=0.143, sparse=5.4799), tau=2.692, Train_accy 88.50:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [09:54<01:39, 33.33s/it]Task 2, Epoch 18/20 => Loss 0.149 (clf=0.143, sparse=5.4799), tau=2.692, Train_accy 88.50:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [09:54<01:05, 32.58s/it]Task 2, Epoch 19/20 => Loss 0.141 (clf=0.135, sparse=5.1321), tau=2.606, Train_accy 88.48:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [10:25<01:05, 32.58s/it]Task 2, Epoch 19/20 => Loss 0.141 (clf=0.135, sparse=5.1321), tau=2.606, Train_accy 88.48:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [10:25<00:32, 32.01s/it]Task 2, Epoch 20/20 => Loss 0.152 (clf=0.147, sparse=4.8439), tau=2.523, Train_accy 88.96:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [10:56<00:32, 32.01s/it]Task 2, Epoch 20/20 => Loss 0.152 (clf=0.147, sparse=4.8439), tau=2.523, Train_accy 88.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:56<00:00, 31.75s/it]Task 2, Epoch 20/20 => Loss 0.152 (clf=0.147, sparse=4.8439), tau=2.523, Train_accy 88.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [10:56<00:00, 32.81s/it]
2025-11-05 21:34:28,292 [sdlora.py] => Task 2, Epoch 20/20 => Loss 0.152 (clf=0.147, sparse=4.8439), tau=2.523, Train_accy 88.96
2025-11-05 21:34:28,294 [sdlora.py] => 
[Conditional Growth] Task 2 - Beta values: [0.90464175 0.         0.09535825]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 21:34:28,294 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 21:34:28,294 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 2 (beta below threshold)
2025-11-05 21:34:28,295 [sdlora.py] => [Conditional Growth] âœ— Pruning NEW adapter for task 2 (beta=0.0954 <= 0.1)
2025-11-05 21:34:28,295 [sdlora.py] => [Conditional Growth] â†’ New adapter deemed not useful, will not be stored
2025-11-05 21:34:28,295 [sdlora.py] => [Conditional Growth] Summary: 1 kept, 2 pruned this round
2025-11-05 21:34:28,295 [sdlora.py] => [Conditional Growth] Active adapters: 1/3 (33.3%)
2025-11-05 21:34:28,295 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 2 adapters pruned!
2025-11-05 21:34:28,308 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_2.pt
2025-11-05 21:34:37,909 [trainer.py] => No NME accuracy.
2025-11-05 21:34:37,909 [trainer.py] => CNN: {'total': np.float64(3.33), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), 'old': np.float64(5.0), 'new': np.float64(0.0)}
2025-11-05 21:34:37,909 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33)]
2025-11-05 21:34:37,909 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67)]

Average Accuracy (CNN): 66.22666666666667
2025-11-05 21:34:37,909 [trainer.py] => Average Accuracy (CNN): 66.22666666666667 

task 3
2025-11-05 21:34:37,911 [trainer.py] => All params: 171989062
2025-11-05 21:34:37,912 [trainer.py] => Trainable params: 391750
2025-11-05 21:34:37,913 [sdlora.py] => Learning on 30-40
2025-11-05 21:34:38,788 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 21:34:38,958 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 21:34:39,136 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 3, Epoch 1/20 => Loss 0.713 (clf=0.706, sparse=7.3352), tau=4.828, Train_accy 52.34, Test_accy 89.68:   0%|          | 0/20 [00:45<?, ?it/s]Task 3, Epoch 1/20 => Loss 0.713 (clf=0.706, sparse=7.3352), tau=4.828, Train_accy 52.34, Test_accy 89.68:   5%|â–Œ         | 1/20 [00:45<14:22, 45.42s/it]Task 3, Epoch 2/20 => Loss 0.344 (clf=0.337, sparse=7.2584), tau=4.658, Train_accy 76.64:   5%|â–Œ         | 1/20 [01:17<14:22, 45.42s/it]                 Task 3, Epoch 2/20 => Loss 0.344 (clf=0.337, sparse=7.2584), tau=4.658, Train_accy 76.64:  10%|â–ˆ         | 2/20 [01:17<11:16, 37.60s/it]Task 3, Epoch 3/20 => Loss 0.298 (clf=0.291, sparse=7.3970), tau=4.495, Train_accy 77.44:  10%|â–ˆ         | 2/20 [01:49<11:16, 37.60s/it]Task 3, Epoch 3/20 => Loss 0.298 (clf=0.291, sparse=7.3970), tau=4.495, Train_accy 77.44:  15%|â–ˆâ–Œ        | 3/20 [01:49<09:58, 35.20s/it]Task 3, Epoch 4/20 => Loss 0.280 (clf=0.272, sparse=7.5368), tau=4.338, Train_accy 77.12:  15%|â–ˆâ–Œ        | 3/20 [02:22<09:58, 35.20s/it]Task 3, Epoch 4/20 => Loss 0.280 (clf=0.272, sparse=7.5368), tau=4.338, Train_accy 77.12:  20%|â–ˆâ–ˆ        | 4/20 [02:22<09:03, 34.00s/it]Task 3, Epoch 5/20 => Loss 0.247 (clf=0.240, sparse=7.1292), tau=4.188, Train_accy 78.10:  20%|â–ˆâ–ˆ        | 4/20 [02:54<09:03, 34.00s/it]Task 3, Epoch 5/20 => Loss 0.247 (clf=0.240, sparse=7.1292), tau=4.188, Train_accy 78.10:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:54<08:20, 33.34s/it]Task 3, Epoch 6/20 => Loss 0.239 (clf=0.232, sparse=7.5129), tau=4.043, Train_accy 78.44, Test_accy 92.50:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:39<08:20, 33.34s/it]Task 3, Epoch 6/20 => Loss 0.239 (clf=0.232, sparse=7.5129), tau=4.043, Train_accy 78.44, Test_accy 92.50:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [03:39<08:44, 37.48s/it]Task 3, Epoch 7/20 => Loss 0.240 (clf=0.233, sparse=7.0681), tau=3.904, Train_accy 79.40:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [04:11<08:44, 37.48s/it]                 Task 3, Epoch 7/20 => Loss 0.240 (clf=0.233, sparse=7.0681), tau=3.904, Train_accy 79.40:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [04:11<07:44, 35.73s/it]Task 3, Epoch 8/20 => Loss 0.245 (clf=0.238, sparse=7.1071), tau=3.770, Train_accy 79.08:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [04:43<07:44, 35.73s/it]Task 3, Epoch 8/20 => Loss 0.245 (clf=0.238, sparse=7.1071), tau=3.770, Train_accy 79.08:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [04:43<06:54, 34.58s/it]Task 3, Epoch 9/20 => Loss 0.249 (clf=0.242, sparse=7.0497), tau=3.642, Train_accy 81.26:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [05:16<06:54, 34.58s/it]Task 3, Epoch 9/20 => Loss 0.249 (clf=0.242, sparse=7.0497), tau=3.642, Train_accy 81.26:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [05:16<06:12, 33.86s/it]Task 3, Epoch 10/20 => Loss 0.247 (clf=0.240, sparse=6.9517), tau=3.519, Train_accy 80.42:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [05:48<06:12, 33.86s/it]Task 3, Epoch 10/20 => Loss 0.247 (clf=0.240, sparse=6.9517), tau=3.519, Train_accy 80.42:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [05:48<05:33, 33.38s/it]Task 3, Epoch 11/20 => Loss 0.213 (clf=0.206, sparse=7.1183), tau=3.400, Train_accy 82.46, Test_accy 93.02:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [06:33<05:33, 33.38s/it]Task 3, Epoch 11/20 => Loss 0.213 (clf=0.206, sparse=7.1183), tau=3.400, Train_accy 82.46, Test_accy 93.02:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [06:33<05:33, 37.06s/it]Task 3, Epoch 12/20 => Loss 0.236 (clf=0.228, sparse=7.7062), tau=3.287, Train_accy 82.68:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [07:06<05:33, 37.06s/it]                 Task 3, Epoch 12/20 => Loss 0.236 (clf=0.228, sparse=7.7062), tau=3.287, Train_accy 82.68:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [07:06<04:44, 35.55s/it]Task 3, Epoch 13/20 => Loss 0.233 (clf=0.225, sparse=7.4487), tau=3.177, Train_accy 81.24:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [07:38<04:44, 35.55s/it]Task 3, Epoch 13/20 => Loss 0.233 (clf=0.225, sparse=7.4487), tau=3.177, Train_accy 81.24:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [07:38<04:02, 34.59s/it]Task 3, Epoch 14/20 => Loss 0.219 (clf=0.212, sparse=7.0204), tau=3.072, Train_accy 83.10:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [08:10<04:02, 34.59s/it]Task 3, Epoch 14/20 => Loss 0.219 (clf=0.212, sparse=7.0204), tau=3.072, Train_accy 83.10:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [08:10<03:23, 33.85s/it]Task 3, Epoch 15/20 => Loss 0.213 (clf=0.206, sparse=7.3611), tau=2.971, Train_accy 84.30:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [08:42<03:23, 33.85s/it]Task 3, Epoch 15/20 => Loss 0.213 (clf=0.206, sparse=7.3611), tau=2.971, Train_accy 84.30:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [08:42<02:46, 33.35s/it]Task 3, Epoch 16/20 => Loss 0.201 (clf=0.194, sparse=6.9783), tau=2.874, Train_accy 84.86, Test_accy 92.62:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [09:28<02:46, 33.35s/it]Task 3, Epoch 16/20 => Loss 0.201 (clf=0.194, sparse=6.9783), tau=2.874, Train_accy 84.86, Test_accy 92.62:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [09:28<02:28, 37.03s/it]Task 3, Epoch 17/20 => Loss 0.180 (clf=0.173, sparse=7.4359), tau=2.781, Train_accy 85.72:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [10:00<02:28, 37.03s/it]                 Task 3, Epoch 17/20 => Loss 0.180 (clf=0.173, sparse=7.4359), tau=2.781, Train_accy 85.72:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [10:00<01:46, 35.67s/it]Task 3, Epoch 18/20 => Loss 0.208 (clf=0.200, sparse=7.2797), tau=2.692, Train_accy 85.40:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [10:33<01:46, 35.67s/it]Task 3, Epoch 18/20 => Loss 0.208 (clf=0.200, sparse=7.2797), tau=2.692, Train_accy 85.40:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [10:33<01:09, 34.65s/it]Task 3, Epoch 19/20 => Loss 0.206 (clf=0.200, sparse=6.8970), tau=2.606, Train_accy 84.92:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [11:05<01:09, 34.65s/it]Task 3, Epoch 19/20 => Loss 0.206 (clf=0.200, sparse=6.8970), tau=2.606, Train_accy 84.92:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [11:05<00:33, 33.95s/it]Task 3, Epoch 20/20 => Loss 0.180 (clf=0.173, sparse=7.1727), tau=2.523, Train_accy 86.46:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [11:37<00:33, 33.95s/it]Task 3, Epoch 20/20 => Loss 0.180 (clf=0.173, sparse=7.1727), tau=2.523, Train_accy 86.46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [11:37<00:00, 33.31s/it]Task 3, Epoch 20/20 => Loss 0.180 (clf=0.173, sparse=7.1727), tau=2.523, Train_accy 86.46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [11:37<00:00, 34.86s/it]
2025-11-05 21:46:16,426 [sdlora.py] => Task 3, Epoch 20/20 => Loss 0.180 (clf=0.173, sparse=7.1727), tau=2.523, Train_accy 86.46
2025-11-05 21:46:16,428 [sdlora.py] => 
[Conditional Growth] Task 3 - Beta values: [0.64674526 0.         0.15766288 0.1955918 ]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 21:46:16,428 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 21:46:16,428 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 2 (beta above threshold)
[GumbelGate] Keeping task 3 (beta above threshold)
2025-11-05 21:46:16,429 [sdlora.py] => [Conditional Growth] âœ“ Keeping NEW adapter for task 3 (beta=0.1956 > 0.1)
2025-11-05 21:46:16,429 [sdlora.py] => [Conditional Growth] Summary: 3 kept, 1 pruned this round
2025-11-05 21:46:16,429 [sdlora.py] => [Conditional Growth] Active adapters: 3/4 (75.0%)
2025-11-05 21:46:16,429 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 1 adapters pruned!
2025-11-05 21:46:16,447 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_3.pt
2025-11-05 21:46:28,933 [trainer.py] => No NME accuracy.
2025-11-05 21:46:28,933 [trainer.py] => CNN: {'total': np.float64(2.5), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), '30-39': np.float64(0.0), 'old': np.float64(3.33), 'new': np.float64(0.0)}
2025-11-05 21:46:28,933 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33), np.float64(2.5)]
2025-11-05 21:46:28,933 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67), np.float64(12.5)]

Average Accuracy (CNN): 50.295
2025-11-05 21:46:28,933 [trainer.py] => Average Accuracy (CNN): 50.295 

task 4
2025-11-05 21:46:28,935 [trainer.py] => All params: 171996752
2025-11-05 21:46:28,936 [trainer.py] => Trainable params: 399440
2025-11-05 21:46:28,937 [sdlora.py] => Learning on 40-50
2025-11-05 21:46:29,811 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 21:46:29,985 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 21:46:30,169 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 4, Epoch 1/20 => Loss 0.601 (clf=0.593, sparse=8.2901), tau=4.828, Train_accy 49.58, Test_accy 88.02:   0%|          | 0/20 [00:49<?, ?it/s]Task 4, Epoch 1/20 => Loss 0.601 (clf=0.593, sparse=8.2901), tau=4.828, Train_accy 49.58, Test_accy 88.02:   5%|â–Œ         | 1/20 [00:49<15:49, 49.97s/it]Task 4, Epoch 2/20 => Loss 0.279 (clf=0.271, sparse=7.8666), tau=4.658, Train_accy 77.64:   5%|â–Œ         | 1/20 [01:23<15:49, 49.97s/it]                 Task 4, Epoch 2/20 => Loss 0.279 (clf=0.271, sparse=7.8666), tau=4.658, Train_accy 77.64:  10%|â–ˆ         | 2/20 [01:23<12:08, 40.50s/it]Task 4, Epoch 3/20 => Loss 0.243 (clf=0.234, sparse=8.1587), tau=4.495, Train_accy 76.46:  10%|â–ˆ         | 2/20 [01:57<12:08, 40.50s/it]Task 4, Epoch 3/20 => Loss 0.243 (clf=0.234, sparse=8.1587), tau=4.495, Train_accy 76.46:  15%|â–ˆâ–Œ        | 3/20 [01:57<10:33, 37.24s/it]Task 4, Epoch 4/20 => Loss 0.222 (clf=0.214, sparse=8.2573), tau=4.338, Train_accy 77.10:  15%|â–ˆâ–Œ        | 3/20 [02:30<10:33, 37.24s/it]Task 4, Epoch 4/20 => Loss 0.222 (clf=0.214, sparse=8.2573), tau=4.338, Train_accy 77.10:  20%|â–ˆâ–ˆ        | 4/20 [02:30<09:33, 35.83s/it]Task 4, Epoch 5/20 => Loss 0.196 (clf=0.188, sparse=8.2240), tau=4.188, Train_accy 77.12:  20%|â–ˆâ–ˆ        | 4/20 [03:04<09:33, 35.83s/it]Task 4, Epoch 5/20 => Loss 0.196 (clf=0.188, sparse=8.2240), tau=4.188, Train_accy 77.12:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:04<08:44, 34.95s/it]Task 4, Epoch 6/20 => Loss 0.189 (clf=0.180, sparse=8.5185), tau=4.043, Train_accy 78.32, Test_accy 90.32:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:54<08:44, 34.95s/it]Task 4, Epoch 6/20 => Loss 0.189 (clf=0.180, sparse=8.5185), tau=4.043, Train_accy 78.32, Test_accy 90.32:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [03:54<09:23, 40.28s/it]Task 4, Epoch 7/20 => Loss 0.197 (clf=0.189, sparse=8.3156), tau=3.904, Train_accy 79.26:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [04:28<09:23, 40.28s/it]                 Task 4, Epoch 7/20 => Loss 0.197 (clf=0.189, sparse=8.3156), tau=3.904, Train_accy 79.26:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [04:28<08:15, 38.12s/it]Task 4, Epoch 8/20 => Loss 0.184 (clf=0.176, sparse=8.1457), tau=3.770, Train_accy 80.20:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:02<08:15, 38.12s/it]Task 4, Epoch 8/20 => Loss 0.184 (clf=0.176, sparse=8.1457), tau=3.770, Train_accy 80.20:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [05:02<07:20, 36.73s/it]Task 4, Epoch 9/20 => Loss 0.189 (clf=0.181, sparse=7.9636), tau=3.642, Train_accy 79.68:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [05:36<07:20, 36.73s/it]Task 4, Epoch 9/20 => Loss 0.189 (clf=0.181, sparse=7.9636), tau=3.642, Train_accy 79.68:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [05:36<06:33, 35.79s/it]Task 4, Epoch 10/20 => Loss 0.164 (clf=0.156, sparse=8.1436), tau=3.519, Train_accy 80.64:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [06:09<06:33, 35.79s/it]Task 4, Epoch 10/20 => Loss 0.164 (clf=0.156, sparse=8.1436), tau=3.519, Train_accy 80.64:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [06:09<05:50, 35.08s/it]Task 4, Epoch 11/20 => Loss 0.170 (clf=0.162, sparse=7.8741), tau=3.400, Train_accy 81.34, Test_accy 90.46:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [06:59<05:50, 35.08s/it]Task 4, Epoch 11/20 => Loss 0.170 (clf=0.162, sparse=7.8741), tau=3.400, Train_accy 81.34, Test_accy 90.46:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [06:59<05:56, 39.66s/it]Task 4, Epoch 12/20 => Loss 0.172 (clf=0.164, sparse=8.6177), tau=3.287, Train_accy 81.68:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [07:33<05:56, 39.66s/it]                 Task 4, Epoch 12/20 => Loss 0.172 (clf=0.164, sparse=8.6177), tau=3.287, Train_accy 81.68:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [07:33<05:02, 37.80s/it]Task 4, Epoch 13/20 => Loss 0.179 (clf=0.171, sparse=7.9305), tau=3.177, Train_accy 81.16:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [08:06<05:02, 37.80s/it]Task 4, Epoch 13/20 => Loss 0.179 (clf=0.171, sparse=7.9305), tau=3.177, Train_accy 81.16:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [08:06<04:15, 36.56s/it]Task 4, Epoch 14/20 => Loss 0.182 (clf=0.174, sparse=7.9640), tau=3.072, Train_accy 81.26:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [08:40<04:15, 36.56s/it]Task 4, Epoch 14/20 => Loss 0.182 (clf=0.174, sparse=7.9640), tau=3.072, Train_accy 81.26:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [08:40<03:34, 35.77s/it]Task 4, Epoch 15/20 => Loss 0.169 (clf=0.160, sparse=8.5531), tau=2.971, Train_accy 81.32:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [09:14<03:34, 35.77s/it]Task 4, Epoch 15/20 => Loss 0.169 (clf=0.160, sparse=8.5531), tau=2.971, Train_accy 81.32:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [09:14<02:55, 35.12s/it]Task 4, Epoch 16/20 => Loss 0.170 (clf=0.162, sparse=7.7563), tau=2.874, Train_accy 82.42, Test_accy 90.74:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [10:04<02:55, 35.12s/it]Task 4, Epoch 16/20 => Loss 0.170 (clf=0.162, sparse=7.7563), tau=2.874, Train_accy 82.42, Test_accy 90.74:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [10:04<02:38, 39.58s/it]Task 4, Epoch 17/20 => Loss 0.170 (clf=0.162, sparse=8.0876), tau=2.781, Train_accy 82.72:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [10:37<02:38, 39.58s/it]                 Task 4, Epoch 17/20 => Loss 0.170 (clf=0.162, sparse=8.0876), tau=2.781, Train_accy 82.72:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [10:37<01:53, 37.75s/it]Task 4, Epoch 18/20 => Loss 0.165 (clf=0.157, sparse=8.0667), tau=2.692, Train_accy 83.62:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [11:11<01:53, 37.75s/it]Task 4, Epoch 18/20 => Loss 0.165 (clf=0.157, sparse=8.0667), tau=2.692, Train_accy 83.62:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [11:11<01:13, 36.53s/it]Task 4, Epoch 19/20 => Loss 0.164 (clf=0.156, sparse=7.9045), tau=2.606, Train_accy 83.74:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [11:45<01:13, 36.53s/it]Task 4, Epoch 19/20 => Loss 0.164 (clf=0.156, sparse=7.9045), tau=2.606, Train_accy 83.74:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [11:45<00:35, 35.69s/it]Task 4, Epoch 20/20 => Loss 0.171 (clf=0.163, sparse=7.9701), tau=2.523, Train_accy 82.00:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [12:18<00:35, 35.69s/it]Task 4, Epoch 20/20 => Loss 0.171 (clf=0.163, sparse=7.9701), tau=2.523, Train_accy 82.00: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [12:18<00:00, 35.06s/it]Task 4, Epoch 20/20 => Loss 0.171 (clf=0.163, sparse=7.9701), tau=2.523, Train_accy 82.00: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [12:18<00:00, 36.94s/it]
2025-11-05 21:58:49,013 [sdlora.py] => Task 4, Epoch 20/20 => Loss 0.171 (clf=0.163, sparse=7.9701), tau=2.523, Train_accy 82.00
2025-11-05 21:58:49,015 [sdlora.py] => 
[Conditional Growth] Task 4 - Beta values: [0.477887   0.         0.16540135 0.18965617 0.16705552]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 21:58:49,016 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 21:58:49,016 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 2 (beta above threshold)
[GumbelGate] Keeping task 3 (beta above threshold)
[GumbelGate] Keeping task 4 (beta above threshold)
2025-11-05 21:58:49,016 [sdlora.py] => [Conditional Growth] âœ“ Keeping NEW adapter for task 4 (beta=0.1671 > 0.1)
2025-11-05 21:58:49,016 [sdlora.py] => [Conditional Growth] Summary: 4 kept, 1 pruned this round
2025-11-05 21:58:49,016 [sdlora.py] => [Conditional Growth] Active adapters: 4/5 (80.0%)
2025-11-05 21:58:49,016 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 1 adapters pruned!
2025-11-05 21:58:49,025 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_4.pt
2025-11-05 21:59:04,925 [trainer.py] => No NME accuracy.
2025-11-05 21:59:04,926 [trainer.py] => CNN: {'total': np.float64(2.0), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), '30-39': np.float64(0.0), '40-49': np.float64(0.0), 'old': np.float64(2.5), 'new': np.float64(0.0)}
2025-11-05 21:59:04,926 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33), np.float64(2.5), np.float64(2.0)]
2025-11-05 21:59:04,926 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67), np.float64(12.5), np.float64(10.0)]

Average Accuracy (CNN): 40.636
2025-11-05 21:59:04,926 [trainer.py] => Average Accuracy (CNN): 40.636 

task 5
2025-11-05 21:59:04,928 [trainer.py] => All params: 172004442
2025-11-05 21:59:04,929 [trainer.py] => Trainable params: 407130
2025-11-05 21:59:04,930 [sdlora.py] => Learning on 50-60
2025-11-05 21:59:05,822 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 21:59:05,997 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 21:59:06,191 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 5, Epoch 1/20 => Loss 0.713 (clf=0.705, sparse=8.7424), tau=4.828, Train_accy 49.42, Test_accy 85.63:   0%|          | 0/20 [00:55<?, ?it/s]Task 5, Epoch 1/20 => Loss 0.713 (clf=0.705, sparse=8.7424), tau=4.828, Train_accy 49.42, Test_accy 85.63:   5%|â–Œ         | 1/20 [00:55<17:31, 55.35s/it]Task 5, Epoch 2/20 => Loss 0.333 (clf=0.324, sparse=9.0899), tau=4.658, Train_accy 77.78:   5%|â–Œ         | 1/20 [01:30<17:31, 55.35s/it]                 Task 5, Epoch 2/20 => Loss 0.333 (clf=0.324, sparse=9.0899), tau=4.658, Train_accy 77.78:  10%|â–ˆ         | 2/20 [01:30<13:01, 43.42s/it]Task 5, Epoch 3/20 => Loss 0.302 (clf=0.293, sparse=8.3294), tau=4.495, Train_accy 76.98:  10%|â–ˆ         | 2/20 [02:05<13:01, 43.42s/it]Task 5, Epoch 3/20 => Loss 0.302 (clf=0.293, sparse=8.3294), tau=4.495, Train_accy 76.98:  15%|â–ˆâ–Œ        | 3/20 [02:05<11:11, 39.50s/it]Task 5, Epoch 4/20 => Loss 0.269 (clf=0.260, sparse=8.7267), tau=4.338, Train_accy 77.64:  15%|â–ˆâ–Œ        | 3/20 [02:40<11:11, 39.50s/it]Task 5, Epoch 4/20 => Loss 0.269 (clf=0.260, sparse=8.7267), tau=4.338, Train_accy 77.64:  20%|â–ˆâ–ˆ        | 4/20 [02:40<10:03, 37.70s/it]Task 5, Epoch 5/20 => Loss 0.249 (clf=0.240, sparse=8.4241), tau=4.188, Train_accy 77.48:  20%|â–ˆâ–ˆ        | 4/20 [03:15<10:03, 37.70s/it]Task 5, Epoch 5/20 => Loss 0.249 (clf=0.240, sparse=8.4241), tau=4.188, Train_accy 77.48:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:15<09:10, 36.67s/it]Task 5, Epoch 6/20 => Loss 0.253 (clf=0.245, sparse=8.8312), tau=4.043, Train_accy 77.70, Test_accy 88.87:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [04:10<09:10, 36.67s/it]Task 5, Epoch 6/20 => Loss 0.253 (clf=0.245, sparse=8.8312), tau=4.043, Train_accy 77.70, Test_accy 88.87:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [04:10<10:02, 43.03s/it]Task 5, Epoch 7/20 => Loss 0.258 (clf=0.249, sparse=8.6102), tau=3.904, Train_accy 78.08:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [04:45<10:02, 43.03s/it]                 Task 5, Epoch 7/20 => Loss 0.258 (clf=0.249, sparse=8.6102), tau=3.904, Train_accy 78.08:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [04:45<08:45, 40.39s/it]Task 5, Epoch 8/20 => Loss 0.218 (clf=0.209, sparse=8.4522), tau=3.770, Train_accy 78.74:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:20<08:45, 40.39s/it]Task 5, Epoch 8/20 => Loss 0.218 (clf=0.209, sparse=8.4522), tau=3.770, Train_accy 78.74:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [05:20<07:43, 38.64s/it]Task 5, Epoch 9/20 => Loss 0.215 (clf=0.206, sparse=8.8688), tau=3.642, Train_accy 79.02:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [05:55<07:43, 38.64s/it]Task 5, Epoch 9/20 => Loss 0.215 (clf=0.206, sparse=8.8688), tau=3.642, Train_accy 79.02:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [05:55<06:52, 37.49s/it]Task 5, Epoch 10/20 => Loss 0.236 (clf=0.227, sparse=8.8032), tau=3.519, Train_accy 78.28:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [06:30<06:52, 37.49s/it]Task 5, Epoch 10/20 => Loss 0.236 (clf=0.227, sparse=8.8032), tau=3.519, Train_accy 78.28:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [06:30<06:07, 36.74s/it]Task 5, Epoch 11/20 => Loss 0.252 (clf=0.243, sparse=8.8084), tau=3.400, Train_accy 77.88, Test_accy 89.03:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [07:25<06:07, 36.74s/it]Task 5, Epoch 11/20 => Loss 0.252 (clf=0.243, sparse=8.8084), tau=3.400, Train_accy 77.88, Test_accy 89.03:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [07:25<06:21, 42.34s/it]Task 5, Epoch 12/20 => Loss 0.247 (clf=0.239, sparse=8.3687), tau=3.287, Train_accy 77.84:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [08:00<06:21, 42.34s/it]                 Task 5, Epoch 12/20 => Loss 0.247 (clf=0.239, sparse=8.3687), tau=3.287, Train_accy 77.84:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [08:00<05:20, 40.11s/it]Task 5, Epoch 13/20 => Loss 0.209 (clf=0.200, sparse=8.3423), tau=3.177, Train_accy 81.00:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [08:35<05:20, 40.11s/it]Task 5, Epoch 13/20 => Loss 0.209 (clf=0.200, sparse=8.3423), tau=3.177, Train_accy 81.00:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [08:35<04:30, 38.59s/it]Task 5, Epoch 14/20 => Loss 0.221 (clf=0.212, sparse=8.5946), tau=3.072, Train_accy 80.74:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [09:10<04:30, 38.59s/it]Task 5, Epoch 14/20 => Loss 0.221 (clf=0.212, sparse=8.5946), tau=3.072, Train_accy 80.74:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [09:10<03:45, 37.52s/it]Task 5, Epoch 15/20 => Loss 0.205 (clf=0.197, sparse=8.2012), tau=2.971, Train_accy 81.26:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [09:45<03:45, 37.52s/it]Task 5, Epoch 15/20 => Loss 0.205 (clf=0.197, sparse=8.2012), tau=2.971, Train_accy 81.26:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [09:45<03:03, 36.70s/it]Task 5, Epoch 16/20 => Loss 0.216 (clf=0.207, sparse=8.5484), tau=2.874, Train_accy 81.66, Test_accy 89.33:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [10:40<03:03, 36.70s/it]Task 5, Epoch 16/20 => Loss 0.216 (clf=0.207, sparse=8.5484), tau=2.874, Train_accy 81.66, Test_accy 89.33:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [10:40<02:48, 42.18s/it]Task 5, Epoch 17/20 => Loss 0.201 (clf=0.193, sparse=8.5943), tau=2.781, Train_accy 82.48:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [11:14<02:48, 42.18s/it]                 Task 5, Epoch 17/20 => Loss 0.201 (clf=0.193, sparse=8.5943), tau=2.781, Train_accy 82.48:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [11:14<01:59, 39.96s/it]Task 5, Epoch 18/20 => Loss 0.206 (clf=0.198, sparse=8.3967), tau=2.692, Train_accy 82.32:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [11:49<01:59, 39.96s/it]Task 5, Epoch 18/20 => Loss 0.206 (clf=0.198, sparse=8.3967), tau=2.692, Train_accy 82.32:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [11:49<01:16, 38.43s/it]Task 5, Epoch 19/20 => Loss 0.194 (clf=0.185, sparse=8.8078), tau=2.606, Train_accy 82.28:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [12:24<01:16, 38.43s/it]Task 5, Epoch 19/20 => Loss 0.194 (clf=0.185, sparse=8.8078), tau=2.606, Train_accy 82.28:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [12:24<00:37, 37.32s/it]Task 5, Epoch 20/20 => Loss 0.197 (clf=0.188, sparse=9.1228), tau=2.523, Train_accy 83.20:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [12:59<00:37, 37.32s/it]Task 5, Epoch 20/20 => Loss 0.197 (clf=0.188, sparse=9.1228), tau=2.523, Train_accy 83.20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [12:59<00:00, 36.61s/it]Task 5, Epoch 20/20 => Loss 0.197 (clf=0.188, sparse=9.1228), tau=2.523, Train_accy 83.20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [12:59<00:00, 38.98s/it]
2025-11-05 22:12:05,705 [sdlora.py] => Task 5, Epoch 20/20 => Loss 0.197 (clf=0.188, sparse=9.1228), tau=2.523, Train_accy 83.20
2025-11-05 22:12:05,707 [sdlora.py] => 
[Conditional Growth] Task 5 - Beta values: [0.3817099  0.         0.09651142 0.15441391 0.21041045 0.15695436]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 2 (beta below threshold)
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 2 (beta=0.0965 <= 0.1)
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] â†’ Task 2 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 3 (beta above threshold)
[GumbelGate] Keeping task 4 (beta above threshold)
[GumbelGate] Keeping task 5 (beta above threshold)
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] âœ“ Keeping NEW adapter for task 5 (beta=0.1570 > 0.1)
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] Summary: 4 kept, 2 pruned this round
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] Active adapters: 4/6 (66.7%)
2025-11-05 22:12:05,708 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 2 adapters pruned!
2025-11-05 22:12:05,734 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_5.pt
2025-11-05 22:12:25,131 [trainer.py] => No NME accuracy.
2025-11-05 22:12:25,132 [trainer.py] => CNN: {'total': np.float64(1.67), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), '30-39': np.float64(0.0), '40-49': np.float64(0.0), '50-59': np.float64(0.0), 'old': np.float64(2.0), 'new': np.float64(0.0)}
2025-11-05 22:12:25,132 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33), np.float64(2.5), np.float64(2.0), np.float64(1.67)]
2025-11-05 22:12:25,132 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67), np.float64(12.5), np.float64(10.0), np.float64(8.33)]

Average Accuracy (CNN): 34.141666666666666
2025-11-05 22:12:25,132 [trainer.py] => Average Accuracy (CNN): 34.141666666666666 

task 6
2025-11-05 22:12:25,134 [trainer.py] => All params: 172012132
2025-11-05 22:12:25,135 [trainer.py] => Trainable params: 414820
2025-11-05 22:12:25,136 [sdlora.py] => Learning on 60-70
2025-11-05 22:12:26,025 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 22:12:26,186 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 22:12:26,489 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 6, Epoch 1/20 => Loss 0.550 (clf=0.541, sparse=9.0637), tau=4.828, Train_accy 55.38, Test_accy 86.46:   0%|          | 0/20 [01:00<?, ?it/s]Task 6, Epoch 1/20 => Loss 0.550 (clf=0.541, sparse=9.0637), tau=4.828, Train_accy 55.38, Test_accy 86.46:   5%|â–Œ         | 1/20 [01:00<19:05, 60.31s/it]Task 6, Epoch 2/20 => Loss 0.245 (clf=0.236, sparse=8.7088), tau=4.658, Train_accy 79.32:   5%|â–Œ         | 1/20 [01:36<19:05, 60.31s/it]                 Task 6, Epoch 2/20 => Loss 0.245 (clf=0.236, sparse=8.7088), tau=4.658, Train_accy 79.32:  10%|â–ˆ         | 2/20 [01:36<13:52, 46.25s/it]Task 6, Epoch 3/20 => Loss 0.228 (clf=0.220, sparse=8.2929), tau=4.495, Train_accy 79.56:  10%|â–ˆ         | 2/20 [02:12<13:52, 46.25s/it]Task 6, Epoch 3/20 => Loss 0.228 (clf=0.220, sparse=8.2929), tau=4.495, Train_accy 79.56:  15%|â–ˆâ–Œ        | 3/20 [02:12<11:47, 41.62s/it]Task 6, Epoch 4/20 => Loss 0.205 (clf=0.196, sparse=8.9938), tau=4.338, Train_accy 78.68:  15%|â–ˆâ–Œ        | 3/20 [02:49<11:47, 41.62s/it]Task 6, Epoch 4/20 => Loss 0.205 (clf=0.196, sparse=8.9938), tau=4.338, Train_accy 78.68:  20%|â–ˆâ–ˆ        | 4/20 [02:49<10:33, 39.61s/it]Task 6, Epoch 5/20 => Loss 0.176 (clf=0.167, sparse=8.4968), tau=4.188, Train_accy 79.98:  20%|â–ˆâ–ˆ        | 4/20 [03:25<10:33, 39.61s/it]Task 6, Epoch 5/20 => Loss 0.176 (clf=0.167, sparse=8.4968), tau=4.188, Train_accy 79.98:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:25<09:35, 38.40s/it]Task 6, Epoch 6/20 => Loss 0.191 (clf=0.182, sparse=8.6098), tau=4.043, Train_accy 80.70, Test_accy 87.91:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [04:25<09:35, 38.40s/it]Task 6, Epoch 6/20 => Loss 0.191 (clf=0.182, sparse=8.6098), tau=4.043, Train_accy 80.70, Test_accy 87.91:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [04:25<10:41, 45.79s/it]Task 6, Epoch 7/20 => Loss 0.168 (clf=0.159, sparse=8.6945), tau=3.904, Train_accy 80.24:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [05:02<10:41, 45.79s/it]                 Task 6, Epoch 7/20 => Loss 0.168 (clf=0.159, sparse=8.6945), tau=3.904, Train_accy 80.24:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:02<09:14, 42.68s/it]Task 6, Epoch 8/20 => Loss 0.193 (clf=0.184, sparse=8.9306), tau=3.770, Train_accy 81.38:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:38<09:14, 42.68s/it]Task 6, Epoch 8/20 => Loss 0.193 (clf=0.184, sparse=8.9306), tau=3.770, Train_accy 81.38:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [05:38<08:08, 40.71s/it]Task 6, Epoch 9/20 => Loss 0.179 (clf=0.170, sparse=8.7923), tau=3.642, Train_accy 81.86:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [06:14<08:08, 40.71s/it]Task 6, Epoch 9/20 => Loss 0.179 (clf=0.170, sparse=8.7923), tau=3.642, Train_accy 81.86:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [06:14<07:12, 39.36s/it]Task 6, Epoch 10/20 => Loss 0.156 (clf=0.147, sparse=8.9243), tau=3.519, Train_accy 82.26:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [06:51<07:12, 39.36s/it]Task 6, Epoch 10/20 => Loss 0.156 (clf=0.147, sparse=8.9243), tau=3.519, Train_accy 82.26:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [06:51<06:24, 38.42s/it]Task 6, Epoch 11/20 => Loss 0.168 (clf=0.159, sparse=8.9930), tau=3.400, Train_accy 82.52, Test_accy 88.74:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [07:51<06:24, 38.42s/it]Task 6, Epoch 11/20 => Loss 0.168 (clf=0.159, sparse=8.9930), tau=3.400, Train_accy 82.52, Test_accy 88.74:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [07:51<06:46, 45.14s/it]Task 6, Epoch 12/20 => Loss 0.181 (clf=0.172, sparse=8.9156), tau=3.287, Train_accy 82.28:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [08:28<06:46, 45.14s/it]                 Task 6, Epoch 12/20 => Loss 0.181 (clf=0.172, sparse=8.9156), tau=3.287, Train_accy 82.28:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [08:28<05:39, 42.50s/it]Task 6, Epoch 13/20 => Loss 0.172 (clf=0.163, sparse=8.5118), tau=3.177, Train_accy 82.32:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [09:04<05:39, 42.50s/it]Task 6, Epoch 13/20 => Loss 0.172 (clf=0.163, sparse=8.5118), tau=3.177, Train_accy 82.32:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [09:04<04:44, 40.67s/it]Task 6, Epoch 14/20 => Loss 0.154 (clf=0.145, sparse=8.7593), tau=3.072, Train_accy 84.28:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [09:40<04:44, 40.67s/it]Task 6, Epoch 14/20 => Loss 0.154 (clf=0.145, sparse=8.7593), tau=3.072, Train_accy 84.28:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [09:40<03:56, 39.38s/it]Task 6, Epoch 15/20 => Loss 0.168 (clf=0.159, sparse=8.5452), tau=2.971, Train_accy 83.28:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [10:17<03:56, 39.38s/it]Task 6, Epoch 15/20 => Loss 0.168 (clf=0.159, sparse=8.5452), tau=2.971, Train_accy 83.28:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [10:17<03:12, 38.50s/it]Task 6, Epoch 16/20 => Loss 0.171 (clf=0.162, sparse=9.1836), tau=2.874, Train_accy 83.86, Test_accy 88.73:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [11:17<03:12, 38.50s/it]Task 6, Epoch 16/20 => Loss 0.171 (clf=0.162, sparse=9.1836), tau=2.874, Train_accy 83.86, Test_accy 88.73:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [11:17<03:00, 45.01s/it]Task 6, Epoch 17/20 => Loss 0.165 (clf=0.156, sparse=9.1090), tau=2.781, Train_accy 83.86:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [11:53<03:00, 45.01s/it]                 Task 6, Epoch 17/20 => Loss 0.165 (clf=0.156, sparse=9.1090), tau=2.781, Train_accy 83.86:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [11:53<02:07, 42.41s/it]Task 6, Epoch 18/20 => Loss 0.154 (clf=0.146, sparse=8.0626), tau=2.692, Train_accy 83.30:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [12:30<02:07, 42.41s/it]Task 6, Epoch 18/20 => Loss 0.154 (clf=0.146, sparse=8.0626), tau=2.692, Train_accy 83.30:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [12:30<01:21, 40.62s/it]Task 6, Epoch 19/20 => Loss 0.158 (clf=0.149, sparse=9.2329), tau=2.606, Train_accy 83.70:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [13:06<01:21, 40.62s/it]Task 6, Epoch 19/20 => Loss 0.158 (clf=0.149, sparse=9.2329), tau=2.606, Train_accy 83.70:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [13:06<00:39, 39.36s/it]Task 6, Epoch 20/20 => Loss 0.173 (clf=0.163, sparse=9.4648), tau=2.523, Train_accy 83.94:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [13:42<00:39, 39.36s/it]Task 6, Epoch 20/20 => Loss 0.173 (clf=0.163, sparse=9.4648), tau=2.523, Train_accy 83.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [13:42<00:00, 38.42s/it]Task 6, Epoch 20/20 => Loss 0.173 (clf=0.163, sparse=9.4648), tau=2.523, Train_accy 83.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [13:42<00:00, 41.15s/it]
2025-11-05 22:26:09,476 [sdlora.py] => Task 6, Epoch 20/20 => Loss 0.173 (clf=0.163, sparse=9.4648), tau=2.523, Train_accy 83.94
2025-11-05 22:26:09,478 [sdlora.py] => 
[Conditional Growth] Task 6 - Beta values: [0.34091774 0.         0.09033101 0.05248512 0.18854263 0.19300431
 0.13471916]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 22:26:09,478 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 22:26:09,478 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 2 (beta below threshold)
2025-11-05 22:26:09,478 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 2 (beta=0.0903 <= 0.1)
2025-11-05 22:26:09,478 [sdlora.py] => [Conditional Growth] â†’ Task 2 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 3 (beta below threshold)
2025-11-05 22:26:09,478 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 3 (beta=0.0525 <= 0.1)
2025-11-05 22:26:09,479 [sdlora.py] => [Conditional Growth] â†’ Task 3 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 4 (beta above threshold)
[GumbelGate] Keeping task 5 (beta above threshold)
[GumbelGate] Keeping task 6 (beta above threshold)
2025-11-05 22:26:09,479 [sdlora.py] => [Conditional Growth] âœ“ Keeping NEW adapter for task 6 (beta=0.1347 > 0.1)
2025-11-05 22:26:09,479 [sdlora.py] => [Conditional Growth] Summary: 4 kept, 3 pruned this round
2025-11-05 22:26:09,479 [sdlora.py] => [Conditional Growth] Active adapters: 4/7 (57.1%)
2025-11-05 22:26:09,479 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 3 adapters pruned!
2025-11-05 22:26:09,496 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_6.pt
2025-11-05 22:26:32,610 [trainer.py] => No NME accuracy.
2025-11-05 22:26:32,610 [trainer.py] => CNN: {'total': np.float64(1.43), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), '30-39': np.float64(0.0), '40-49': np.float64(0.0), '50-59': np.float64(0.0), '60-69': np.float64(0.0), 'old': np.float64(1.67), 'new': np.float64(0.0)}
2025-11-05 22:26:32,610 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33), np.float64(2.5), np.float64(2.0), np.float64(1.67), np.float64(1.43)]
2025-11-05 22:26:32,610 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67), np.float64(12.5), np.float64(10.0), np.float64(8.33), np.float64(7.14)]

Average Accuracy (CNN): 29.46857142857143
2025-11-05 22:26:32,610 [trainer.py] => Average Accuracy (CNN): 29.46857142857143 

task 7
2025-11-05 22:26:32,612 [trainer.py] => All params: 172019822
2025-11-05 22:26:32,613 [trainer.py] => Trainable params: 422510
2025-11-05 22:26:32,614 [sdlora.py] => Learning on 70-80
2025-11-05 22:26:33,499 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 22:26:33,661 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 22:26:33,866 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 7, Epoch 1/20 => Loss 0.688 (clf=0.679, sparse=8.7062), tau=4.828, Train_accy 39.62, Test_accy 84.04:   0%|          | 0/20 [01:05<?, ?it/s]Task 7, Epoch 1/20 => Loss 0.688 (clf=0.679, sparse=8.7062), tau=4.828, Train_accy 39.62, Test_accy 84.04:   5%|â–Œ         | 1/20 [01:05<20:47, 65.66s/it]Task 7, Epoch 2/20 => Loss 0.348 (clf=0.339, sparse=9.2557), tau=4.658, Train_accy 66.02:   5%|â–Œ         | 1/20 [01:43<20:47, 65.66s/it]                 Task 7, Epoch 2/20 => Loss 0.348 (clf=0.339, sparse=9.2557), tau=4.658, Train_accy 66.02:  10%|â–ˆ         | 2/20 [01:43<14:46, 49.26s/it]Task 7, Epoch 3/20 => Loss 0.343 (clf=0.334, sparse=8.9778), tau=4.495, Train_accy 64.48:  10%|â–ˆ         | 2/20 [02:21<14:46, 49.26s/it]Task 7, Epoch 3/20 => Loss 0.343 (clf=0.334, sparse=8.9778), tau=4.495, Train_accy 64.48:  15%|â–ˆâ–Œ        | 3/20 [02:21<12:28, 44.02s/it]Task 7, Epoch 4/20 => Loss 0.314 (clf=0.304, sparse=9.4767), tau=4.338, Train_accy 63.20:  15%|â–ˆâ–Œ        | 3/20 [02:59<12:28, 44.02s/it]Task 7, Epoch 4/20 => Loss 0.314 (clf=0.304, sparse=9.4767), tau=4.338, Train_accy 63.20:  20%|â–ˆâ–ˆ        | 4/20 [02:59<11:04, 41.56s/it]Task 7, Epoch 5/20 => Loss 0.251 (clf=0.242, sparse=9.3455), tau=4.188, Train_accy 64.74:  20%|â–ˆâ–ˆ        | 4/20 [03:36<11:04, 41.56s/it]Task 7, Epoch 5/20 => Loss 0.251 (clf=0.242, sparse=9.3455), tau=4.188, Train_accy 64.74:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:36<10:02, 40.16s/it]Task 7, Epoch 6/20 => Loss 0.237 (clf=0.228, sparse=9.4014), tau=4.043, Train_accy 68.08, Test_accy 86.25:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [04:42<10:02, 40.16s/it]Task 7, Epoch 6/20 => Loss 0.237 (clf=0.228, sparse=9.4014), tau=4.043, Train_accy 68.08, Test_accy 86.25:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [04:42<11:23, 48.81s/it]Task 7, Epoch 7/20 => Loss 0.236 (clf=0.227, sparse=9.1916), tau=3.904, Train_accy 65.78:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [05:19<11:23, 48.81s/it]                 Task 7, Epoch 7/20 => Loss 0.236 (clf=0.227, sparse=9.1916), tau=3.904, Train_accy 65.78:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:19<09:46, 45.13s/it]Task 7, Epoch 8/20 => Loss 0.230 (clf=0.220, sparse=9.5155), tau=3.770, Train_accy 67.14:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:57<09:46, 45.13s/it]Task 7, Epoch 8/20 => Loss 0.230 (clf=0.220, sparse=9.5155), tau=3.770, Train_accy 67.14:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [05:57<08:33, 42.81s/it]Task 7, Epoch 9/20 => Loss 0.248 (clf=0.239, sparse=8.9491), tau=3.642, Train_accy 69.12:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [06:35<08:33, 42.81s/it]Task 7, Epoch 9/20 => Loss 0.248 (clf=0.239, sparse=8.9491), tau=3.642, Train_accy 69.12:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [06:35<07:32, 41.18s/it]Task 7, Epoch 10/20 => Loss 0.255 (clf=0.246, sparse=9.1518), tau=3.519, Train_accy 67.92:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [07:12<07:32, 41.18s/it]Task 7, Epoch 10/20 => Loss 0.255 (clf=0.246, sparse=9.1518), tau=3.519, Train_accy 67.92:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [07:12<06:41, 40.11s/it]Task 7, Epoch 11/20 => Loss 0.225 (clf=0.216, sparse=9.1752), tau=3.400, Train_accy 69.16, Test_accy 87.70:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [08:18<06:41, 40.11s/it]Task 7, Epoch 11/20 => Loss 0.225 (clf=0.216, sparse=9.1752), tau=3.400, Train_accy 69.16, Test_accy 87.70:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [08:18<07:10, 47.87s/it]Task 7, Epoch 12/20 => Loss 0.233 (clf=0.224, sparse=9.1457), tau=3.287, Train_accy 70.78:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [08:56<07:10, 47.87s/it]                 Task 7, Epoch 12/20 => Loss 0.233 (clf=0.224, sparse=9.1457), tau=3.287, Train_accy 70.78:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [08:56<05:59, 44.88s/it]Task 7, Epoch 13/20 => Loss 0.221 (clf=0.212, sparse=9.1142), tau=3.177, Train_accy 72.06:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [09:34<05:59, 44.88s/it]Task 7, Epoch 13/20 => Loss 0.221 (clf=0.212, sparse=9.1142), tau=3.177, Train_accy 72.06:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [09:34<04:59, 42.77s/it]Task 7, Epoch 14/20 => Loss 0.206 (clf=0.197, sparse=9.3523), tau=3.072, Train_accy 73.40:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [10:12<04:59, 42.77s/it]Task 7, Epoch 14/20 => Loss 0.206 (clf=0.197, sparse=9.3523), tau=3.072, Train_accy 73.40:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [10:12<04:07, 41.22s/it]Task 7, Epoch 15/20 => Loss 0.212 (clf=0.202, sparse=9.5331), tau=2.971, Train_accy 73.16:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [10:49<04:07, 41.22s/it]Task 7, Epoch 15/20 => Loss 0.212 (clf=0.202, sparse=9.5331), tau=2.971, Train_accy 73.16:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [10:49<03:20, 40.19s/it]Task 7, Epoch 16/20 => Loss 0.210 (clf=0.200, sparse=9.2988), tau=2.874, Train_accy 73.88, Test_accy 87.95:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [11:55<03:20, 40.19s/it]Task 7, Epoch 16/20 => Loss 0.210 (clf=0.200, sparse=9.2988), tau=2.874, Train_accy 73.88, Test_accy 87.95:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [11:55<03:11, 47.81s/it]Task 7, Epoch 17/20 => Loss 0.198 (clf=0.189, sparse=9.0121), tau=2.781, Train_accy 74.14:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [12:33<03:11, 47.81s/it]                 Task 7, Epoch 17/20 => Loss 0.198 (clf=0.189, sparse=9.0121), tau=2.781, Train_accy 74.14:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [12:33<02:14, 44.86s/it]Task 7, Epoch 18/20 => Loss 0.193 (clf=0.184, sparse=9.0559), tau=2.692, Train_accy 75.84:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [13:11<02:14, 44.86s/it]Task 7, Epoch 18/20 => Loss 0.193 (clf=0.184, sparse=9.0559), tau=2.692, Train_accy 75.84:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [13:11<01:25, 42.70s/it]Task 7, Epoch 19/20 => Loss 0.215 (clf=0.206, sparse=9.4447), tau=2.606, Train_accy 76.52:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [13:48<01:25, 42.70s/it]Task 7, Epoch 19/20 => Loss 0.215 (clf=0.206, sparse=9.4447), tau=2.606, Train_accy 76.52:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [13:48<00:41, 41.15s/it]Task 7, Epoch 20/20 => Loss 0.201 (clf=0.192, sparse=8.9340), tau=2.523, Train_accy 77.56:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [14:26<00:41, 41.15s/it]Task 7, Epoch 20/20 => Loss 0.201 (clf=0.192, sparse=8.9340), tau=2.523, Train_accy 77.56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [14:26<00:00, 40.09s/it]Task 7, Epoch 20/20 => Loss 0.201 (clf=0.192, sparse=8.9340), tau=2.523, Train_accy 77.56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [14:26<00:00, 43.31s/it]
2025-11-05 22:41:00,069 [sdlora.py] => Task 7, Epoch 20/20 => Loss 0.201 (clf=0.192, sparse=8.9340), tau=2.523, Train_accy 77.56
2025-11-05 22:41:00,071 [sdlora.py] => 
[Conditional Growth] Task 7 - Beta values: [0.30545953 0.         0.04177441 0.1197613  0.1471254  0.18737738
 0.09274206 0.10575998]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 2 (beta below threshold)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 2 (beta=0.0418 <= 0.1)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] â†’ Task 2 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 3 (beta above threshold)
[GumbelGate] Keeping task 4 (beta above threshold)
[GumbelGate] Keeping task 5 (beta above threshold)
[GumbelGate] Pruned task 6 (beta below threshold)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 6 (beta=0.0927 <= 0.1)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] â†’ Task 6 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 7 (beta above threshold)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] âœ“ Keeping NEW adapter for task 7 (beta=0.1058 > 0.1)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] Summary: 5 kept, 3 pruned this round
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] Active adapters: 5/8 (62.5%)
2025-11-05 22:41:00,072 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 3 adapters pruned!
2025-11-05 22:41:00,075 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_7.pt
2025-11-05 22:41:27,066 [trainer.py] => No NME accuracy.
2025-11-05 22:41:27,067 [trainer.py] => CNN: {'total': np.float64(1.25), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), '30-39': np.float64(0.0), '40-49': np.float64(0.0), '50-59': np.float64(0.0), '60-69': np.float64(0.0), '70-79': np.float64(0.0), 'old': np.float64(1.43), 'new': np.float64(0.0)}
2025-11-05 22:41:27,067 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33), np.float64(2.5), np.float64(2.0), np.float64(1.67), np.float64(1.43), np.float64(1.25)]
2025-11-05 22:41:27,067 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67), np.float64(12.5), np.float64(10.0), np.float64(8.33), np.float64(7.14), np.float64(6.25)]

Average Accuracy (CNN): 25.94125
2025-11-05 22:41:27,067 [trainer.py] => Average Accuracy (CNN): 25.94125 

task 8
2025-11-05 22:41:27,069 [trainer.py] => All params: 172027512
2025-11-05 22:41:27,070 [trainer.py] => Trainable params: 430200
2025-11-05 22:41:27,071 [sdlora.py] => Learning on 80-90
2025-11-05 22:41:27,960 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 22:41:28,127 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 22:41:28,337 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 8, Epoch 1/20 => Loss 0.685 (clf=0.676, sparse=9.2515), tau=4.828, Train_accy 42.04, Test_accy 82.82:   0%|          | 0/20 [01:10<?, ?it/s]Task 8, Epoch 1/20 => Loss 0.685 (clf=0.676, sparse=9.2515), tau=4.828, Train_accy 42.04, Test_accy 82.82:   5%|â–Œ         | 1/20 [01:10<22:25, 70.81s/it]Task 8, Epoch 2/20 => Loss 0.284 (clf=0.276, sparse=8.7942), tau=4.658, Train_accy 70.20:   5%|â–Œ         | 1/20 [01:49<22:25, 70.81s/it]                 Task 8, Epoch 2/20 => Loss 0.284 (clf=0.276, sparse=8.7942), tau=4.658, Train_accy 70.20:  10%|â–ˆ         | 2/20 [01:49<15:37, 52.09s/it]Task 8, Epoch 3/20 => Loss 0.301 (clf=0.292, sparse=9.9010), tau=4.495, Train_accy 69.26:  10%|â–ˆ         | 2/20 [02:29<15:37, 52.09s/it]Task 8, Epoch 3/20 => Loss 0.301 (clf=0.292, sparse=9.9010), tau=4.495, Train_accy 69.26:  15%|â–ˆâ–Œ        | 3/20 [02:29<13:06, 46.24s/it]Task 8, Epoch 4/20 => Loss 0.269 (clf=0.260, sparse=9.1307), tau=4.338, Train_accy 71.22:  15%|â–ˆâ–Œ        | 3/20 [03:08<13:06, 46.24s/it]Task 8, Epoch 4/20 => Loss 0.269 (clf=0.260, sparse=9.1307), tau=4.338, Train_accy 71.22:  20%|â–ˆâ–ˆ        | 4/20 [03:08<11:35, 43.46s/it]Task 8, Epoch 5/20 => Loss 0.230 (clf=0.221, sparse=9.5944), tau=4.188, Train_accy 73.00:  20%|â–ˆâ–ˆ        | 4/20 [03:47<11:35, 43.46s/it]Task 8, Epoch 5/20 => Loss 0.230 (clf=0.221, sparse=9.5944), tau=4.188, Train_accy 73.00:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:47<10:27, 41.85s/it]Task 8, Epoch 6/20 => Loss 0.236 (clf=0.227, sparse=9.2795), tau=4.043, Train_accy 72.32, Test_accy 85.51:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [04:58<10:27, 41.85s/it]Task 8, Epoch 6/20 => Loss 0.236 (clf=0.227, sparse=9.2795), tau=4.043, Train_accy 72.32, Test_accy 85.51:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [04:58<12:03, 51.70s/it]Task 8, Epoch 7/20 => Loss 0.228 (clf=0.219, sparse=9.0757), tau=3.904, Train_accy 72.56:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [05:37<12:03, 51.70s/it]                 Task 8, Epoch 7/20 => Loss 0.228 (clf=0.219, sparse=9.0757), tau=3.904, Train_accy 72.56:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:37<10:18, 47.59s/it]Task 8, Epoch 8/20 => Loss 0.204 (clf=0.195, sparse=8.6998), tau=3.770, Train_accy 73.54:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [06:16<10:18, 47.59s/it]Task 8, Epoch 8/20 => Loss 0.204 (clf=0.195, sparse=8.6998), tau=3.770, Train_accy 73.54:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [06:16<08:58, 44.88s/it]Task 8, Epoch 9/20 => Loss 0.220 (clf=0.211, sparse=9.5463), tau=3.642, Train_accy 74.20:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [06:55<08:58, 44.88s/it]Task 8, Epoch 9/20 => Loss 0.220 (clf=0.211, sparse=9.5463), tau=3.642, Train_accy 74.20:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [06:55<07:53, 43.06s/it]Task 8, Epoch 10/20 => Loss 0.202 (clf=0.192, sparse=9.3349), tau=3.519, Train_accy 74.08:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [07:34<07:53, 43.06s/it]Task 8, Epoch 10/20 => Loss 0.202 (clf=0.192, sparse=9.3349), tau=3.519, Train_accy 74.08:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [07:34<06:59, 41.91s/it]Task 8, Epoch 11/20 => Loss 0.220 (clf=0.212, sparse=8.3609), tau=3.400, Train_accy 74.10, Test_accy 85.96:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [08:46<06:59, 41.91s/it]Task 8, Epoch 11/20 => Loss 0.220 (clf=0.212, sparse=8.3609), tau=3.400, Train_accy 74.10, Test_accy 85.96:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [08:46<07:38, 50.92s/it]Task 8, Epoch 12/20 => Loss 0.209 (clf=0.200, sparse=9.2837), tau=3.287, Train_accy 74.52:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [09:25<07:38, 50.92s/it]                 Task 8, Epoch 12/20 => Loss 0.209 (clf=0.200, sparse=9.2837), tau=3.287, Train_accy 74.52:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [09:25<06:18, 47.32s/it]Task 8, Epoch 13/20 => Loss 0.226 (clf=0.217, sparse=9.2089), tau=3.177, Train_accy 73.94:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [10:04<06:18, 47.32s/it]Task 8, Epoch 13/20 => Loss 0.226 (clf=0.217, sparse=9.2089), tau=3.177, Train_accy 73.94:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [10:04<05:13, 44.85s/it]Task 8, Epoch 14/20 => Loss 0.212 (clf=0.203, sparse=9.3437), tau=3.072, Train_accy 73.50:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [10:43<05:13, 44.85s/it]Task 8, Epoch 14/20 => Loss 0.212 (clf=0.203, sparse=9.3437), tau=3.072, Train_accy 73.50:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [10:43<04:18, 43.06s/it]Task 8, Epoch 15/20 => Loss 0.184 (clf=0.174, sparse=9.3714), tau=2.971, Train_accy 75.34:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [11:22<04:18, 43.06s/it]Task 8, Epoch 15/20 => Loss 0.184 (clf=0.174, sparse=9.3714), tau=2.971, Train_accy 75.34:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [11:22<03:29, 41.91s/it]Task 8, Epoch 16/20 => Loss 0.205 (clf=0.196, sparse=9.0570), tau=2.874, Train_accy 76.00, Test_accy 86.16:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [12:33<03:29, 41.91s/it]Task 8, Epoch 16/20 => Loss 0.205 (clf=0.196, sparse=9.0570), tau=2.874, Train_accy 76.00, Test_accy 86.16:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [12:33<03:22, 50.56s/it]Task 8, Epoch 17/20 => Loss 0.195 (clf=0.185, sparse=9.6858), tau=2.781, Train_accy 74.92:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [13:12<03:22, 50.56s/it]                 Task 8, Epoch 17/20 => Loss 0.195 (clf=0.185, sparse=9.6858), tau=2.781, Train_accy 74.92:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [13:12<02:21, 47.21s/it]Task 8, Epoch 18/20 => Loss 0.197 (clf=0.188, sparse=9.2212), tau=2.692, Train_accy 76.02:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [13:51<02:21, 47.21s/it]Task 8, Epoch 18/20 => Loss 0.197 (clf=0.188, sparse=9.2212), tau=2.692, Train_accy 76.02:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [13:51<01:29, 44.84s/it]Task 8, Epoch 19/20 => Loss 0.181 (clf=0.171, sparse=9.3250), tau=2.606, Train_accy 77.48:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [14:30<01:29, 44.84s/it]Task 8, Epoch 19/20 => Loss 0.181 (clf=0.171, sparse=9.3250), tau=2.606, Train_accy 77.48:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [14:30<00:43, 43.12s/it]Task 8, Epoch 20/20 => Loss 0.194 (clf=0.184, sparse=9.4848), tau=2.523, Train_accy 77.46:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [15:10<00:43, 43.12s/it]Task 8, Epoch 20/20 => Loss 0.194 (clf=0.184, sparse=9.4848), tau=2.523, Train_accy 77.46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [15:10<00:00, 41.95s/it]Task 8, Epoch 20/20 => Loss 0.194 (clf=0.184, sparse=9.4848), tau=2.523, Train_accy 77.46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [15:10<00:00, 45.51s/it]
2025-11-05 22:56:38,529 [sdlora.py] => Task 8, Epoch 20/20 => Loss 0.194 (clf=0.184, sparse=9.4848), tau=2.523, Train_accy 77.46
2025-11-05 22:56:38,531 [sdlora.py] => 
[Conditional Growth] Task 8 - Beta values: [0.30293673 0.         0.07079606 0.07679022 0.09864857 0.15398642
 0.10452975 0.09089548 0.10141676]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 22:56:38,531 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 22:56:38,531 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 2 (beta below threshold)
2025-11-05 22:56:38,531 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 2 (beta=0.0708 <= 0.1)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] â†’ Task 2 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 3 (beta below threshold)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 3 (beta=0.0768 <= 0.1)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] â†’ Task 3 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 4 (beta below threshold)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 4 (beta=0.0986 <= 0.1)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] â†’ Task 4 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 5 (beta above threshold)
[GumbelGate] Keeping task 6 (beta above threshold)
[GumbelGate] Pruned task 7 (beta below threshold)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 7 (beta=0.0909 <= 0.1)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] â†’ Task 7 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 8 (beta above threshold)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] âœ“ Keeping NEW adapter for task 8 (beta=0.1014 > 0.1)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] Summary: 4 kept, 5 pruned this round
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] Active adapters: 4/9 (44.4%)
2025-11-05 22:56:38,532 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 5 adapters pruned!
2025-11-05 22:56:38,549 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_8.pt
2025-11-05 22:57:09,483 [trainer.py] => No NME accuracy.
2025-11-05 22:57:09,483 [trainer.py] => CNN: {'total': np.float64(1.11), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), '30-39': np.float64(0.0), '40-49': np.float64(0.0), '50-59': np.float64(0.0), '60-69': np.float64(0.0), '70-79': np.float64(0.0), '80-89': np.float64(0.0), 'old': np.float64(1.25), 'new': np.float64(0.0)}
2025-11-05 22:57:09,483 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33), np.float64(2.5), np.float64(2.0), np.float64(1.67), np.float64(1.43), np.float64(1.25), np.float64(1.11)]
2025-11-05 22:57:09,483 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67), np.float64(12.5), np.float64(10.0), np.float64(8.33), np.float64(7.14), np.float64(6.25), np.float64(5.56)]

Average Accuracy (CNN): 23.182222222222222
2025-11-05 22:57:09,483 [trainer.py] => Average Accuracy (CNN): 23.182222222222222 

task 9
2025-11-05 22:57:09,485 [trainer.py] => All params: 172035202
2025-11-05 22:57:09,486 [trainer.py] => Trainable params: 437890
2025-11-05 22:57:09,487 [sdlora.py] => Learning on 90-100
2025-11-05 22:57:10,388 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-11-05 22:57:10,564 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
save_file ./CF100/
2025-11-05 22:57:10,784 [sdlora.py] => [Gumbel CL-LoRA] tau_init=5.0, tau_final=0.5, anneal_rate=0.999, lambda_sparsity=0.001
  0%|          | 0/20 [00:00<?, ?it/s]Task 9, Epoch 1/20 => Loss 0.750 (clf=0.741, sparse=9.2249), tau=4.828, Train_accy 51.60, Test_accy 82.82:   0%|          | 0/20 [01:16<?, ?it/s]Task 9, Epoch 1/20 => Loss 0.750 (clf=0.741, sparse=9.2249), tau=4.828, Train_accy 51.60, Test_accy 82.82:   5%|â–Œ         | 1/20 [01:16<24:18, 76.78s/it]Task 9, Epoch 2/20 => Loss 0.356 (clf=0.346, sparse=10.1903), tau=4.658, Train_accy 76.62:   5%|â–Œ         | 1/20 [01:57<24:18, 76.78s/it]                Task 9, Epoch 2/20 => Loss 0.356 (clf=0.346, sparse=10.1903), tau=4.658, Train_accy 76.62:  10%|â–ˆ         | 2/20 [01:57<16:38, 55.46s/it]Task 9, Epoch 3/20 => Loss 0.312 (clf=0.302, sparse=9.2490), tau=4.495, Train_accy 75.34:  10%|â–ˆ         | 2/20 [02:37<16:38, 55.46s/it] Task 9, Epoch 3/20 => Loss 0.312 (clf=0.302, sparse=9.2490), tau=4.495, Train_accy 75.34:  15%|â–ˆâ–Œ        | 3/20 [02:37<13:47, 48.68s/it]Task 9, Epoch 4/20 => Loss 0.277 (clf=0.268, sparse=9.5766), tau=4.338, Train_accy 75.64:  15%|â–ˆâ–Œ        | 3/20 [03:18<13:47, 48.68s/it]Task 9, Epoch 4/20 => Loss 0.277 (clf=0.268, sparse=9.5766), tau=4.338, Train_accy 75.64:  20%|â–ˆâ–ˆ        | 4/20 [03:18<12:07, 45.48s/it]Task 9, Epoch 5/20 => Loss 0.263 (clf=0.254, sparse=9.0247), tau=4.188, Train_accy 74.54:  20%|â–ˆâ–ˆ        | 4/20 [03:59<12:07, 45.48s/it]Task 9, Epoch 5/20 => Loss 0.263 (clf=0.254, sparse=9.0247), tau=4.188, Train_accy 74.54:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [03:59<10:56, 43.74s/it]Task 9, Epoch 6/20 => Loss 0.270 (clf=0.261, sparse=9.1867), tau=4.043, Train_accy 74.74, Test_accy 85.55:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [05:15<10:56, 43.74s/it]Task 9, Epoch 6/20 => Loss 0.270 (clf=0.261, sparse=9.1867), tau=4.043, Train_accy 74.74, Test_accy 85.55:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [05:15<12:48, 54.91s/it]Task 9, Epoch 7/20 => Loss 0.258 (clf=0.249, sparse=9.6662), tau=3.904, Train_accy 74.08:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [05:56<12:48, 54.91s/it]                 Task 9, Epoch 7/20 => Loss 0.258 (clf=0.249, sparse=9.6662), tau=3.904, Train_accy 74.08:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [05:56<10:52, 50.21s/it]Task 9, Epoch 8/20 => Loss 0.262 (clf=0.253, sparse=9.5972), tau=3.770, Train_accy 75.72:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [06:37<10:52, 50.21s/it]Task 9, Epoch 8/20 => Loss 0.262 (clf=0.253, sparse=9.5972), tau=3.770, Train_accy 75.72:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [06:37<09:26, 47.19s/it]Task 9, Epoch 9/20 => Loss 0.263 (clf=0.254, sparse=9.1861), tau=3.642, Train_accy 74.30:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [07:17<09:26, 47.19s/it]Task 9, Epoch 9/20 => Loss 0.263 (clf=0.254, sparse=9.1861), tau=3.642, Train_accy 74.30:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [07:17<08:16, 45.16s/it]Task 9, Epoch 10/20 => Loss 0.249 (clf=0.239, sparse=9.6112), tau=3.519, Train_accy 74.16:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [07:58<08:16, 45.16s/it]Task 9, Epoch 10/20 => Loss 0.249 (clf=0.239, sparse=9.6112), tau=3.519, Train_accy 74.16:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [07:58<07:17, 43.77s/it]Task 9, Epoch 11/20 => Loss 0.251 (clf=0.242, sparse=8.9391), tau=3.400, Train_accy 74.80, Test_accy 85.64:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [09:15<07:17, 43.77s/it]Task 9, Epoch 11/20 => Loss 0.251 (clf=0.242, sparse=8.9391), tau=3.400, Train_accy 74.80, Test_accy 85.64:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [09:15<08:05, 53.91s/it]Task 9, Epoch 12/20 => Loss 0.243 (clf=0.234, sparse=9.2497), tau=3.287, Train_accy 75.78:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [09:55<08:05, 53.91s/it]                 Task 9, Epoch 12/20 => Loss 0.243 (clf=0.234, sparse=9.2497), tau=3.287, Train_accy 75.78:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [09:55<06:39, 49.88s/it]Task 9, Epoch 13/20 => Loss 0.237 (clf=0.227, sparse=9.1999), tau=3.177, Train_accy 76.18:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [10:36<06:39, 49.88s/it]Task 9, Epoch 13/20 => Loss 0.237 (clf=0.227, sparse=9.1999), tau=3.177, Train_accy 76.18:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [10:36<05:29, 47.05s/it]Task 9, Epoch 14/20 => Loss 0.252 (clf=0.242, sparse=9.3248), tau=3.072, Train_accy 76.82:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [11:16<05:29, 47.05s/it]Task 9, Epoch 14/20 => Loss 0.252 (clf=0.242, sparse=9.3248), tau=3.072, Train_accy 76.82:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [11:16<04:30, 45.05s/it]Task 9, Epoch 15/20 => Loss 0.228 (clf=0.219, sparse=9.2927), tau=2.971, Train_accy 78.14:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [11:57<04:30, 45.05s/it]Task 9, Epoch 15/20 => Loss 0.228 (clf=0.219, sparse=9.2927), tau=2.971, Train_accy 78.14:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [11:57<03:38, 43.76s/it]Task 9, Epoch 16/20 => Loss 0.218 (clf=0.209, sparse=8.9302), tau=2.874, Train_accy 78.50, Test_accy 85.68:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [13:14<03:38, 43.76s/it]Task 9, Epoch 16/20 => Loss 0.218 (clf=0.209, sparse=8.9302), tau=2.874, Train_accy 78.50, Test_accy 85.68:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [13:14<03:35, 53.79s/it]Task 9, Epoch 17/20 => Loss 0.209 (clf=0.200, sparse=9.0071), tau=2.781, Train_accy 78.78:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [13:55<03:35, 53.79s/it]                 Task 9, Epoch 17/20 => Loss 0.209 (clf=0.200, sparse=9.0071), tau=2.781, Train_accy 78.78:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [13:55<02:29, 49.86s/it]Task 9, Epoch 18/20 => Loss 0.206 (clf=0.197, sparse=9.3796), tau=2.692, Train_accy 78.76:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [14:35<02:29, 49.86s/it]Task 9, Epoch 18/20 => Loss 0.206 (clf=0.197, sparse=9.3796), tau=2.692, Train_accy 78.76:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [14:35<01:34, 47.01s/it]Task 9, Epoch 19/20 => Loss 0.213 (clf=0.204, sparse=9.1449), tau=2.606, Train_accy 78.80:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [15:16<01:34, 47.01s/it]Task 9, Epoch 19/20 => Loss 0.213 (clf=0.204, sparse=9.1449), tau=2.606, Train_accy 78.80:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [15:16<00:45, 45.15s/it]Task 9, Epoch 20/20 => Loss 0.188 (clf=0.179, sparse=9.2086), tau=2.523, Train_accy 80.30:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [15:57<00:45, 45.15s/it]Task 9, Epoch 20/20 => Loss 0.188 (clf=0.179, sparse=9.2086), tau=2.523, Train_accy 80.30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [15:57<00:00, 43.79s/it]Task 9, Epoch 20/20 => Loss 0.188 (clf=0.179, sparse=9.2086), tau=2.523, Train_accy 80.30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [15:57<00:00, 47.86s/it]
2025-11-05 23:13:08,052 [sdlora.py] => Task 9, Epoch 20/20 => Loss 0.188 (clf=0.179, sparse=9.2086), tau=2.523, Train_accy 80.30
2025-11-05 23:13:08,054 [sdlora.py] => 
[Conditional Growth] Task 9 - Beta values: [0.223393   0.         0.13484049 0.06318118 0.0692025  0.09315238
 0.07230669 0.0835396  0.16167878 0.09870538]
[GumbelGate] Keeping task 0 (beta above threshold)
[GumbelGate] Pruned task 1 (beta below threshold)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 1 (beta=0.0000 <= 0.1)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] â†’ Task 1 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 2 (beta above threshold)
[GumbelGate] Pruned task 3 (beta below threshold)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 3 (beta=0.0632 <= 0.1)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] â†’ Task 3 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 4 (beta below threshold)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 4 (beta=0.0692 <= 0.1)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] â†’ Task 4 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 5 (beta below threshold)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 5 (beta=0.0932 <= 0.1)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] â†’ Task 5 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 6 (beta below threshold)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 6 (beta=0.0723 <= 0.1)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] â†’ Task 6 adapter no longer useful, pruned retroactively
[GumbelGate] Pruned task 7 (beta below threshold)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] âœ— Pruning OLD adapter for task 7 (beta=0.0835 <= 0.1)
2025-11-05 23:13:08,055 [sdlora.py] => [Conditional Growth] â†’ Task 7 adapter no longer useful, pruned retroactively
[GumbelGate] Keeping task 8 (beta above threshold)
[GumbelGate] Pruned task 9 (beta below threshold)
2025-11-05 23:13:08,056 [sdlora.py] => [Conditional Growth] âœ— Pruning NEW adapter for task 9 (beta=0.0987 <= 0.1)
2025-11-05 23:13:08,056 [sdlora.py] => [Conditional Growth] â†’ New adapter deemed not useful, will not be stored
2025-11-05 23:13:08,056 [sdlora.py] => [Conditional Growth] Summary: 3 kept, 7 pruned this round
2025-11-05 23:13:08,056 [sdlora.py] => [Conditional Growth] Active adapters: 3/10 (30.0%)
2025-11-05 23:13:08,056 [sdlora.py] => [Conditional Growth] ðŸŽ¯ Sublinear growth achieved: 7 adapters pruned!
2025-11-05 23:13:08,071 [sdlora.py] => [Conditional Growth] Beta values saved to ./CF100/beta_values_task_9.pt
2025-11-05 23:13:43,518 [trainer.py] => No NME accuracy.
2025-11-05 23:13:43,518 [trainer.py] => CNN: {'total': np.float64(1.0), '00-09': np.float64(10.0), '10-19': np.float64(0.0), '20-29': np.float64(0.0), '30-39': np.float64(0.0), '40-49': np.float64(0.0), '50-59': np.float64(0.0), '60-69': np.float64(0.0), '70-79': np.float64(0.0), '80-89': np.float64(0.0), '90-99': np.float64(0.0), 'old': np.float64(1.11), 'new': np.float64(0.0)}
2025-11-05 23:13:43,518 [trainer.py] => CNN top1 curve: [np.float64(99.0), np.float64(96.35), np.float64(3.33), np.float64(2.5), np.float64(2.0), np.float64(1.67), np.float64(1.43), np.float64(1.25), np.float64(1.11), np.float64(1.0)]
2025-11-05 23:13:43,518 [trainer.py] => CNN top5 curve: [np.float64(100.0), np.float64(99.85), np.float64(16.67), np.float64(12.5), np.float64(10.0), np.float64(8.33), np.float64(7.14), np.float64(6.25), np.float64(5.56), np.float64(5.0)]

Average Accuracy (CNN): 20.964000000000002
2025-11-05 23:13:43,518 [trainer.py] => Average Accuracy (CNN): 20.964000000000002 

Accuracy Matrix (CNN):
[[99.  97.1 10.  10.  10.  10.  10.  10.  10.  10. ]
 [ 0.  95.6  0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]
 [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]
2025-11-05 23:13:43,519 [trainer.py] => Forgetting (CNN): 20.51111111111111
